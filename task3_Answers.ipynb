{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for  `model_simple_nmt` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simple_nmt(human_vocab_size, machine_vocab_size, Tx = 20):\n",
    "    \"\"\"\n",
    "    Simple Neural Machine Translation model\n",
    "    \n",
    "    Arguments:\n",
    "    human_vocab_size -- size of the human vocabulary for dates, it will give us the size of the embedding layer\n",
    "    machine_vocab_size -- size of the machine vocabulary for dates, it will give us the size of the output vector\n",
    "    \n",
    "    Returns:\n",
    "    model -- model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define the input of your model with a shape (Tx,)\n",
    "    inputs = Input(shape=(Tx,))\n",
    "    \n",
    "    # Define the embedding layer. Embedding dimension should be 64 and input_length set to Tx.\n",
    "    # Remember that the size of the embedding is a hyper-parameters!  Choose it to be whatever gives good results.\n",
    "    input_embed = Embedding(human_vocab_size, 64, input_length = Tx)(inputs)\n",
    "    \n",
    "    # Encode the embeddings using a bidirectional LSTM\n",
    "    enc_out = Bidirectional(LSTM(32, return_sequences=True))(input_embed)\n",
    "    \n",
    "    # Decode the encoding using an LSTM layer\n",
    "    dec_out = LSTM(32, return_sequences=True)(enc_out)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Apply Dense layer to every time step\n",
    "    output = TimeDistributed(Dense(machine_vocab_size, activation='softmax'))(dec_out)\n",
    "    \n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for  `attention_3d_block` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    \"\"\"\n",
    "    Implement the attention block applied between two layers\n",
    "    \n",
    "    Argument:\n",
    "    inputs -- output of the previous layer, set of hidden states\n",
    "    \n",
    "    Returns:\n",
    "    output_attention_mul -- inputs weighted with attention probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_h and Tx from inputs' shape. Recall: inputs.shape = (m, Tx, n_h)\n",
    "    Tx = int_shape(inputs)[1]\n",
    "    n_h = int_shape(inputs)[2]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Permute inputs' columns to compute \"a\" of shape (m, n_h, Tx)\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    \n",
    "    # Apply a Dense layer with softmax activation. It should contain Tx neurons. a.shape should still be (m, n_h, Tx).\n",
    "    a = Dense(Tx, activation='softmax')(a)\n",
    "    \n",
    "    # Compute the mean of \"a\" over axis=1 (the \"hidden\" axis: n_h). a.shape should now be (m, Tx)\n",
    "    a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Repeat the vector \"a\" n_h times. \"a\" should now be of shape (m, n_h, Tx)\n",
    "    a = RepeatVector(n_h)(a)\n",
    "    \n",
    "    # Permute the 2nd and the first column of a to get a probability vector of attention. a_probs.shape = (m, Tx, n_h)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    \n",
    "    # Apply the attention probabilities to the \"inputs\" by multiplying element-wise.\n",
    "    output_attention_mul = Multiply(name='attention_mul')([inputs, a_probs])\n",
    "\n",
    "    \n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution for  `model_attention_nmt` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_attention_nmt(human_vocab_size, machine_vocab_size, Tx = 20):\n",
    "    \"\"\"\n",
    "    Attention Neural Machine Translation model\n",
    "    \n",
    "    Arguments:\n",
    "    human_vocab_size -- size of the human vocabulary for dates, it will give us the size of the embedding layer\n",
    "    machine_vocab_size -- size of the machine vocabulary for dates, it will give us the size of the output vector\n",
    "    \n",
    "    Returns:\n",
    "    model -- model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the input of your model with a shape (Tx,)\n",
    "    inputs = Input(shape=(Tx,))\n",
    "    \n",
    "    # Define the embedding layer. This layer should be trainable and the input_length should be set to Tx.\n",
    "    input_embed = Embedding(human_vocab_size, 2*32, input_length = Tx, trainable=True)(inputs)\n",
    "\n",
    "    ### START CODE HERE ###    \n",
    "    # Encode the embeddings using a bidirectional LSTM\n",
    "    enc_out = Bidirectional(LSTM(32, return_sequences=True))(input_embed)\n",
    "    \n",
    "    # Add attention\n",
    "    attention = attention_3d_block(enc_out)\n",
    "    \n",
    "    # Decode the encoding using an LSTM layer\n",
    "    dec_out = LSTM(32, return_sequences=True)(attention)\n",
    " \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Apply Dense layer to every time steps\n",
    "    output = TimeDistributed(Dense(machine_vocab_size, activation='softmax'))(dec_out)\n",
    "    \n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
