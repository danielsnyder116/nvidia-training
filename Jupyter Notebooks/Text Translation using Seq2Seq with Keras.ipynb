{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key use case of Natural Language Processing (NLP) - often regarded as the 'Holy Grail' of the field - is the ability to have a machine automatically translate text from one language to another.  This field is known as Machine Translation (MT) and when using Neural Networks and Deep Learning, it is usually called Neural Machine Translation (NMT).\n",
    "\n",
    "Translation between languages is often difficult, even for a human.  Languages differ from each other in many ways, some more subtle than others.  An entire field of study - [Linguistic Typology](https://en.wikipedia.org/wiki/Linguistic_typology) - is dedicated to the understanding of how languages compare to each other.  As such, translation requires excellent knowledge of both source and target languages and their vagaries.\n",
    "\n",
    "With the huge amounts of textual data available today, coupled with the compute power afforded by GPUs, it is only natural that NMT will see a huge rise in usage.  In fact, a number of well-known companies offer various products in this area.\n",
    "\n",
    "In this lab we will focus on the following:\n",
    "\n",
    "1. Building a Neural Machine Translation for converting human-readable date formats to machine-readable date formats\n",
    "2. Improving the model using the concept of attention\n",
    "3. Measuring the results of Neural Machine Translation using BLEU score\n",
    "4. Real-world issues with Neural Machine Translation - Byte-Pair Encoding and Beam Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation - Some Background\n",
    "\n",
    "Machine translation belongs to a class of problems known as *Sequence-to-Sequence*. As the name implies, these problems deal with converting a *source sequence* $X$ into a *target sequence* $Y$.  Framing machine translation as a Sequence-to-Sequence (Seq-to-Seq) problem suggests that $X$ contains tokens (words or individual characters) in a source *language* while $Y$ are the equivalent tokens in the *target* language. Note that in the most general case, $X$ and $Y$ may have different lengths and equivalent tokens may even be in different locations, depending on the underlying word order of the languages involved.  \n",
    "\n",
    "In the case of NMT, we would like to learn a function $f$ that maximizes the probability of a target sequence $Y$ given the source sequence $X$.  More formally:  given $X = \\{x_1, x_2, ..., x_n\\}$ and $Y = \\{y_1, y_2, ..., y_m\\}$ we would like $f$ to maximize the following conditional probability:\n",
    "\n",
    "$$ P(y_i \\space | \\space y_{i-1}, ..., y_{1}, X) $$\n",
    "\n",
    "for every $y_i$ in $Y$. There are two important points about this expression:\n",
    "1.  We take into account information from the source sequence X.  In fact, we will use a function $g(X)$ which will *encode* X into some useful form.  \n",
    "2.  The value of the current output token $y_i$ depends on all previous output tokens.  This is where the 'sequence' nature of the problem comes from.\n",
    "\n",
    "We will be using a deep neural network to learn this function $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lost in Translation\n",
    "\n",
    "Imagine we are tasked with translating from a language such as German to English.\n",
    "\n",
    "Below are 5 different results from different translation models. Although we don't know what the original sentence is, we can see that context can be easily figured out from the following examples when considered as a group. \n",
    "\n",
    "Translation A: I ask him whether he will once again make a stand-up comedy tour.\n",
    "\n",
    "Translation B: I ask him if he will again make a stand-up comedy tour.\n",
    "\n",
    "Translation C: I wonder him if he will ever make a booth up comedy tour.\n",
    "\n",
    "Translation D: I ask him if he will ever make a stand-up comedy tour ever.\n",
    "\n",
    "Translation E: I ask him whether he will again make a stand-up comedy tour."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Rank the translations from Best to Worst\n",
    "\n",
    "Rank: A E B D C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be relatively easy to spot the worst translation, as it doesn't quite make sense in English. This is the result of a completely *literal* translation that does not take context into account.\n",
    "\n",
    "Finding the worst translation was easy. On the other hand, identifying the best translation might differ from person to person since there's some subjectivity involved. Take Translation D for example: double use of 'ever' in one sentence probably lowers its score as a good English translation. Interestingly, the models producing the better translations all share an architectural detail. This is a key observation, and one that we will return to later when we discuss the *attention mechanism*.\n",
    "\n",
    "\n",
    "Run the following cell to load all the modules we will be using in this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.backend import int_shape\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Setup multiple versions of OpenSeq2Seq\n",
    "\n",
    "import sys\n",
    "from importlib import import_module\n",
    "\n",
    "sys.path.append('old_s2s')\n",
    "old_run = import_module('run')\n",
    "\n",
    "sys.path.remove('old_s2s')\n",
    "sys.path.append('OpenSeq2Seq')\n",
    "new_run = import_module('run2')\n",
    "\n",
    "def run_open_seq2seq(version, args):\n",
    "    args = args.split(' ')\n",
    "    args.insert(0, '')\n",
    "    sys.argv = args\n",
    "    version.main()\n",
    "    \n",
    "    \n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code runs a pre-trained translation model.  Let it run, you can open up [baseline.txt](baseline.txt) to see the translation as it's being written. Once you see \"I wonder him if he will ever make a booth up com@@ ed@@ y@@ tour .\" as the output then you can press Stop button or Interrupt Kernel. It should take about 3.5 minutes to run to reach this line. Simply eliminate the @@ characters to get the sentences used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">==================> Running in inference mode\n",
      ">==================> Executing training mode\n",
      ">==================> Creating data layer\n",
      "WARNING: skipped 0 pairs with max source size of 0 and max target size of 0\n",
      ">==================> Data layer created\n",
      "WARNING:tensorflow:From old_s2s/run.py:207: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      ">==================> Building graph on GPU:0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Inference Mode. Loss part of graph isn't built.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      ">==================> Trying to restore from: /dli/data/noatt/model-59244\n",
      "INFO:tensorflow:Restoring parameters from /dli/data/noatt/model-59244\n",
      ">==================> Saving inference results to: baseline.txt\n",
      "Prime Minister India and Japan met in Tokyo . </S> </S> </S>\n",
      "Federal Government and Bun@@ de@@ stag not joined . </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n",
      "I wonder him if he will ever make a booth up com@@ ed@@ y@@ tour . </S> </S> </S> </S> </S>\n",
      "A@@ il@@ inn Sol@@ om@@ ons was a delicate beauty with un@@ attended hair and a flat heart from a village on the G@@ ordin@@ sel , which was even remote and rough than Port Re@@ ub@@ en . </S> </S>\n",
      "Some gran@@ at@@ ers are still in my kne@@ e . </S>\n",
      "If your camera has a remote mode , using a series of images with slightly changing exposure settings , use it . </S> </S> </S> </S>\n",
      "And those who resist claims know that the time is for them , but certainly not with the pla@@ ins . </S> </S> </S>\n",
      "The villa is probably from the first to the fourth century to Christ until the invasion of the German@@ ans in the Roman R@@ land , suspec@@ ted of Mor@@ schei@@ ser . </S> </S>\n",
      "As Ro@@ d@@ gers was 13 , she believed that she believed that Fried@@ ge A@@ tax@@ ia ( F@@ As ) - a disease with very bad survival . </S> </S> </S>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5864904c5a2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m run_open_seq2seq(\n\u001b[1;32m      2\u001b[0m     \u001b[0mold_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     '--config_file=old_s2s/example_configs/nmt_noatt.json --logdir=/dli/data/noatt --mode=infer --inference_out=baseline.txt')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-405c987fc99e>\u001b[0m in \u001b[0;36mrun_open_seq2seq\u001b[0;34m(version, args)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dli/tasks/task3/task/old_s2s/run.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mdeco_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running in inference mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown mode in config file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dli/tasks/task3/task/old_s2s/run.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            feed_dict={\n\u001b[1;32m    229\u001b[0m                                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                            })\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_out\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"stdout\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    581\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1060\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_open_seq2seq(\n",
    "    old_run, \n",
    "    '--config_file=old_s2s/example_configs/nmt_noatt.json --logdir=/dli/data/noatt --mode=infer --inference_out=baseline.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will begin with the Keras Deep-Learning framework with a Tensorflow backend. We will then switch to using [Open Seq2Seq](https://github.com/NVIDIA/OpenSeq2Seq), an NVIDIA open-source project for training sequence-to-sequence models.  In fact, the above code uses OpenSeq2Seq and by the end of this lab you will also be able to build your own machine translation models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Human-Readable Dates into Machine-Readable Dates\n",
    "\n",
    "We normally use NMT for translation between natural source and target languages.  In this section we will attempt a [toy problem](https://en.wikipedia.org/wiki/Toy_problem) in order to get a better intuition for the concepts involved.\n",
    "\n",
    "Our model translates human-readable dates such as \"the 29th of August 1958\", \"03/30/1968\" and \"24 JUNE 1987\" into a standard, machine-readable form such as \"1958-08-29\", \"1968-03-30\" and \"1987-06-24\".  We will be using a date encoding of YYYY-MM-dd, where months precede days. Make sure that you understand how this problem fits into the framework of both machine translation and sequence-to-sequence. \n",
    "\n",
    "From the File menu above, click Open and select the file nmt_utils.py.  Look in the section titled FORMATS to see the various source text patterns we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is a corpus of 10000 human readable dates and their equivalent machine readable dates. Run the following cell to load the dataset.  In particular, note the `human_vocab` and `machine_vocab` variables.  These define our source and target *vocabularies*, making our problem a character-sequence to character-sequence problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 17823.84it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(m)\n",
    "\n",
    "# Maximum sequence length\n",
    "max_sequence_length = 30\n",
    "\n",
    "\n",
    "sources, targets = zip(*dataset)\n",
    "sources = np.array([string_to_int(i, max_sequence_length, human_vocab) for i in sources])\n",
    "targets = [string_to_int(t, max_sequence_length, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pieces of data have been loaded:\n",
    "\n",
    "- max_sequence_length: the maximum length for a character sequence\n",
    "- dataset: a list of (human-readable date, machine-readable date) pairs\n",
    "- human_vocab: a mapping between all characters that are used in the human-readable dates to their integer index values\n",
    "- machine_vocab: similar to `human_vocab`, a mapping between all characters used in the *machine-readable* dates to their integer index values\n",
    "- inv_machine_vocab: an inverse mapping of `machine_vocab` (i.e., integer to character)\n",
    "- sources: a processed version of the `dataset`'s human-readable dates.  Each character has been replaced by its index from `human_vocab`, and the resulting sequences have been padded to the length specified by `max_sequence_length`. \n",
    "- targets: a processed version of the `dataset`'s machine-readable dates.  Each character has been replaced the its index from `machine_vocab` and the sequences have been padded to the length specified by `max_sequence_length`.  Note also that the values have been 1-hot encoded, in order to use as labels in a model. \n",
    "\n",
    "The following cell selects a random value from `dataset`, and shows the way dates are encoded.  Run multiple times to see different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sources shape          :  (10000, 30)\n",
      "targets shape          :  (10000, 30, 15)\n",
      "Length of human_vocab  :  62\n",
      "Length of machine_vocab:  15\n",
      "\n",
      "\n",
      "Human-readable date                 :  9/14/91\n",
      "Machine-readable date               :  1991-09-14\n",
      "Pre-processed human-readable date   : \n",
      " [37 53  4 31 53 37  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n",
      "Pre-processed machine-readable date : \n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "index = np.random.randint(m)\n",
    "\n",
    "print(\"sources shape          : \", sources.shape)\n",
    "print(\"targets shape          : \", targets.shape)\n",
    "print(\"Length of human_vocab  : \", len(human_vocab))\n",
    "print(\"Length of machine_vocab: \", len(machine_vocab))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Human-readable date                 : \", dataset[index][0])\n",
    "print(\"Machine-readable date               : \", dataset[index][1])\n",
    "print(\"Pre-processed human-readable date   : \\n\", sources[index])\n",
    "print(\"Pre-processed machine-readable date : \\n\", targets[index]) # 1-hot encoded\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before continuing, make sure you understand how the data is encoded.  What happens when the result sequence is shorter than 20 characters?\n",
    "\n",
    "Answer:  The result sequence is then padded to be at least 20 characters long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often-used architecture for seq-to-seq models - and hence, NMT - is known as `encoder-decoder`.  At a high level, the following steps are carried out by this network:\n",
    "\n",
    "1.  Input - in our case, a human-readable date - is fed into the `encoder` part of the network.  The output of the encoder is one or many *state* vectors (also known as *context* vectors).  These state vectors encode a *representation* of the input data.  Each individual timestep may have its own context vector.  Alternatively, we can use the final state vector to represent the entire input.  Using the notation from the background section above:  given a sequence $X = \\{h_1, h_2, ..., h_j\\}$ - where $h_j$ is the output of the encoder DNN -  we can choose $g(X)$ to return all values in $X$ or simply choose $g(X) = h_j$.\n",
    "\n",
    "2.  The output of the encoder is fed into the *decoder* part of the network.  In other words, the decoder receives the *representations* of the input that were learned by the encoder. The output of the decoder is the final output which in our case is the machine-readable date.  The decoder uses the data encoded in the state vectors to create the output.  \n",
    "\n",
    "Since both the inputs and outputs of our model are sequences, it makes sense to use RNN's as the basis of both the encoder and decoder.  We can then train the encoder-decoder model in an end-to-end fashion so that both networks learn the relevant features.\n",
    "\n",
    "### The Encoder Network\n",
    "\n",
    "<img src=\"images/enc.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 1**: NMT Encoder Network </center></caption>\n",
    "\n",
    "As shown in the figure, the input to the encoder is the text of the human-readable date.  Each character in the sequence is then run through an *embedding layer*.  The purpose of the embedding is to convert a sparse, 1-hot representation of the character to a dense representation which learns features about this character.  This representation is then fed into a bidirectional LSTM (Bi-LSTM).  The purpose of the Bi-LSTM is to look at a particular sequences both from front-to-back as well as from back-to-front.  In this way, the network creates a context for each character in the text that depends on both its past as well as its future.  Finally, we take the sequence of hidden states of the Bi-LSTM as our state vector list `enc_out`.\n",
    "\n",
    "### The Decoder Network\n",
    "\n",
    "<img src=\"images/dec.png\" style=\"width:500;height:300px;\"> <br>\n",
    "<caption><center> **Figure 2**: NMT Decoder Network </center></caption>\n",
    "\n",
    "The decoder receives `enc_out` as its input.  This sequence is itself run through an LSTM which produces a set of numbers.  We use the Softmax function to normalize these numbers so that we can treat them as probabilities.  It is then possible to choose the character which has the highest probability as the model output.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the `model_simple_nmt()` described in the figure below. The LSTMs both use 32 units and the embedding dimension is 64. These functions might be useful: [Input()](https://keras.io/layers/core/#input), [Embedding()](https://keras.io/layers/embeddings/), [LSTM()](https://keras.io/layers/recurrent/#lstm), [Bidirectional()](https://keras.io/layers/wrappers/#bidirectional), [Dense()](https://keras.io/layers/core/#dense), [TimeDistributed()](https://keras.io/layers/wrappers/#timedistributed), [Model()](https://keras.io/models/model/).  \n",
    "\n",
    "A solution is available in the file `solutions.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_simple_nmt(human_vocab_size, machine_vocab_size, max_sequence_length = 20):\n",
    "    \"\"\"\n",
    "    Simple Neural Machine Translation model\n",
    "    \n",
    "    Arguments:\n",
    "    human_vocab_size -- size of the human vocabulary for dates, it will give us the size of the embedding layer\n",
    "    machine_vocab_size -- size of the machine vocabulary for dates, it will give us the size of the output vector\n",
    "    \n",
    "    Returns:\n",
    "    model -- model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define the input of your model with a shape (max_sequence_length,)\n",
    "    \n",
    "    inputs = Input((max_sequence_length,), name='input_layer')\n",
    "    \n",
    "    # Define the embedding layer. Embedding dimension should be 64 and input_length set to max_sequence_length.\n",
    "    input_embed = Embedding(human_vocab_size, 64, input_length=max_sequence_length, name='Embedding_Layer')(inputs)\n",
    "    \n",
    "    # Encode the embeddings using a bidirectional LSTM\n",
    "    enc_out = Bidirectional(LSTM(32, return_sequences=True), name='Encoder')(input_embed)\n",
    "    \n",
    "    # Decode the encoder output using an LSTM layer\n",
    "    dec_out = LSTM(32, name='Decoder', return_sequences=True)(enc_out)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Apply Dense layer to every time step\n",
    "    output = TimeDistributed(Dense(machine_vocab_size, activation='softmax'))(dec_out)\n",
    "    \n",
    "    # Create model instance \n",
    "    model = Model(input=[inputs], output=output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your `model` and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 30, 64)            3968      \n",
      "_________________________________________________________________\n",
      "Encoder (Bidirectional)      (None, 30, 64)            24832     \n",
      "_________________________________________________________________\n",
      "Decoder (LSTM)               (None, 30, 32)            12416     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 15)            495       \n",
      "=================================================================\n",
      "Total params: 41,711\n",
      "Trainable params: 41,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = model_simple_nmt(len(human_vocab), len(machine_vocab), max_sequence_length)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# See the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the following cell to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/40\n",
      " - 11s - loss: 1.1571 - acc: 0.6736 - val_loss: 0.8164 - val_acc: 0.7318\n",
      "Epoch 2/40\n",
      " - 9s - loss: 0.7685 - acc: 0.7490 - val_loss: 0.7279 - val_acc: 0.7575\n",
      "Epoch 3/40\n",
      " - 9s - loss: 0.7131 - acc: 0.7677 - val_loss: 0.6908 - val_acc: 0.7738\n",
      "Epoch 4/40\n",
      " - 9s - loss: 0.6832 - acc: 0.7705 - val_loss: 0.6688 - val_acc: 0.7697\n",
      "Epoch 5/40\n",
      " - 9s - loss: 0.6564 - acc: 0.7786 - val_loss: 0.6285 - val_acc: 0.7989\n",
      "Epoch 6/40\n",
      " - 9s - loss: 0.6305 - acc: 0.7935 - val_loss: 0.5986 - val_acc: 0.8020\n",
      "Epoch 7/40\n",
      " - 9s - loss: 0.6000 - acc: 0.8008 - val_loss: 0.5765 - val_acc: 0.8135\n",
      "Epoch 8/40\n",
      " - 9s - loss: 0.5690 - acc: 0.8224 - val_loss: 0.5344 - val_acc: 0.8310\n",
      "Epoch 9/40\n",
      " - 9s - loss: 0.5359 - acc: 0.8257 - val_loss: 0.5034 - val_acc: 0.8334\n",
      "Epoch 10/40\n",
      " - 9s - loss: 0.5071 - acc: 0.8274 - val_loss: 0.4780 - val_acc: 0.8330\n",
      "Epoch 11/40\n",
      " - 9s - loss: 0.4753 - acc: 0.8313 - val_loss: 0.5756 - val_acc: 0.8197\n",
      "Epoch 12/40\n",
      " - 9s - loss: 0.4552 - acc: 0.8324 - val_loss: 0.4302 - val_acc: 0.8394\n",
      "Epoch 13/40\n",
      " - 9s - loss: 0.4326 - acc: 0.8337 - val_loss: 0.4131 - val_acc: 0.8413\n",
      "Epoch 14/40\n",
      " - 9s - loss: 0.4145 - acc: 0.8370 - val_loss: 0.3960 - val_acc: 0.8451\n",
      "Epoch 15/40\n",
      " - 9s - loss: 0.3959 - acc: 0.8430 - val_loss: 0.4038 - val_acc: 0.8418\n",
      "Epoch 16/40\n",
      " - 9s - loss: 0.3777 - acc: 0.8582 - val_loss: 0.3604 - val_acc: 0.8682\n",
      "Epoch 17/40\n",
      " - 9s - loss: 0.3549 - acc: 0.8731 - val_loss: 0.3362 - val_acc: 0.8826\n",
      "Epoch 18/40\n",
      " - 9s - loss: 0.3332 - acc: 0.8811 - val_loss: 0.3274 - val_acc: 0.8793\n",
      "Epoch 19/40\n",
      " - 9s - loss: 0.3212 - acc: 0.8835 - val_loss: 0.3111 - val_acc: 0.8857\n",
      "Epoch 20/40\n",
      " - 9s - loss: 0.3068 - acc: 0.8866 - val_loss: 0.3125 - val_acc: 0.8855\n",
      "Epoch 21/40\n",
      " - 9s - loss: 0.2975 - acc: 0.8892 - val_loss: 0.3060 - val_acc: 0.8861\n",
      "Epoch 22/40\n",
      " - 9s - loss: 0.2889 - acc: 0.8915 - val_loss: 0.2913 - val_acc: 0.8905\n",
      "Epoch 23/40\n",
      " - 9s - loss: 0.2800 - acc: 0.8954 - val_loss: 0.2752 - val_acc: 0.8966\n",
      "Epoch 24/40\n",
      " - 9s - loss: 0.2702 - acc: 0.8993 - val_loss: 0.2722 - val_acc: 0.8969\n",
      "Epoch 25/40\n",
      " - 9s - loss: 0.2624 - acc: 0.9027 - val_loss: 0.2554 - val_acc: 0.9058\n",
      "Epoch 26/40\n",
      " - 9s - loss: 0.2535 - acc: 0.9058 - val_loss: 0.2483 - val_acc: 0.9058\n",
      "Epoch 27/40\n",
      " - 9s - loss: 0.2452 - acc: 0.9088 - val_loss: 0.2430 - val_acc: 0.9063\n",
      "Epoch 28/40\n",
      " - 9s - loss: 0.2378 - acc: 0.9118 - val_loss: 0.2341 - val_acc: 0.9116\n",
      "Epoch 29/40\n",
      " - 9s - loss: 0.2296 - acc: 0.9151 - val_loss: 0.2252 - val_acc: 0.9148\n",
      "Epoch 30/40\n",
      " - 9s - loss: 0.2220 - acc: 0.9181 - val_loss: 0.2228 - val_acc: 0.9151\n",
      "Epoch 31/40\n",
      " - 9s - loss: 0.2140 - acc: 0.9212 - val_loss: 0.2305 - val_acc: 0.9125\n",
      "Epoch 32/40\n",
      " - 9s - loss: 0.2079 - acc: 0.9225 - val_loss: 0.2065 - val_acc: 0.9223\n",
      "Epoch 33/40\n",
      " - 9s - loss: 0.2006 - acc: 0.9253 - val_loss: 0.2049 - val_acc: 0.9228\n",
      "Epoch 34/40\n",
      " - 9s - loss: 0.1948 - acc: 0.9276 - val_loss: 0.1927 - val_acc: 0.9261\n",
      "Epoch 35/40\n",
      " - 9s - loss: 0.1879 - acc: 0.9300 - val_loss: 0.2015 - val_acc: 0.9242\n",
      "Epoch 36/40\n",
      " - 9s - loss: 0.1824 - acc: 0.9330 - val_loss: 0.1843 - val_acc: 0.9320\n",
      "Epoch 37/40\n",
      " - 9s - loss: 0.1761 - acc: 0.9354 - val_loss: 0.1849 - val_acc: 0.9310\n",
      "Epoch 38/40\n",
      " - 9s - loss: 0.1702 - acc: 0.9384 - val_loss: 0.1762 - val_acc: 0.9341\n",
      "Epoch 39/40\n",
      " - 9s - loss: 0.1635 - acc: 0.9410 - val_loss: 0.1704 - val_acc: 0.9377\n",
      "Epoch 40/40\n",
      " - 9s - loss: 0.1591 - acc: 0.9431 - val_loss: 0.1551 - val_acc: 0.9437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4ee085a58>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([sources[:8000]], targets[:8000], epochs=40, batch_size=128, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have good training and validation scores.  However, we measure score for individual *output characters* and not then entire output sequence.  In other words, if a particular digit is in the right place, we consider that a success regardless of other digits in the same output expression.  This is obviously problematic: April 5 is 04-05 and not 03-05.  Nevertheless, *any* date in April - even a wrong one - is probably better than a date in May.  We'll see a better way to measure translation quality later on.\n",
    "\n",
    "In the meantime, let's see the results of our test set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 1s 476us/step\n"
     ]
    }
   ],
   "source": [
    "_,accuracy = model.evaluate([sources[8000:]], targets[8000:], batch_size=128,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9435499997138977\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!  \n",
    "\n",
    "However, as we described above, we are probably not measuring things right.  Let's run some simple variations of a single date through the model to get an intuition for what the final sequences *really* look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 03.3.2000\n",
      "output: 2000-03-00\n",
      "******\n",
      "source: 03 03 2000\n",
      "output: 2000-03-00\n",
      "******\n",
      "source: 03 mar 2000\n",
      "output: 2000-03-03\n",
      "******\n",
      "source: 03 march 2000\n",
      "output: 2000-03-03\n",
      "******\n",
      "source: march 03 2000\n",
      "output: 2000-03-00\n",
      "******\n",
      "source: march 03, 2000\n",
      "output: 2000-03-00\n",
      "******\n",
      "---------\n",
      "source: 04.4.2001\n",
      "output: 2000-04-05\n",
      "******\n",
      "source: 04 04 2001\n",
      "output: 2000-04-05\n",
      "******\n",
      "source: 03 apr 2000\n",
      "output: 2000-03-03\n",
      "******\n",
      "source: 04 april 2001\n",
      "output: 2000-04-05\n",
      "******\n",
      "source: april 04 2001\n",
      "output: 2000-09-04\n",
      "******\n",
      "source: april 04, 2001\n",
      "output: 2000-09-04\n",
      "******\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES1 = ['03.3.2000', '03 03 2000', '03 mar 2000', '03 march 2000', 'march 03 2000', 'march 03, 2000' ]\n",
    "EXAMPLES2 = ['04.4.2001', '04 04 2001', '03 apr 2000', '04 april 2001', 'april 04 2001', 'april 04, 2001' ]\n",
    "\n",
    "def prediction_to_text(model_results):\n",
    "    results = np.argmax(model_results[0], axis=-1)\n",
    "    output = int_to_string(results, inv_machine_vocab)\n",
    "    return ''.join(output).replace('<pad>','')\n",
    "\n",
    "def run_examples(examples):\n",
    "    for example in examples:\n",
    "        print(\"source:\", example)\n",
    "        source = string_to_int(example, max_sequence_length, human_vocab)\n",
    "        prediction = model.predict(np.array([source]))\n",
    "        print(\"output:\", prediction_to_text(prediction))\n",
    "        print('******')\n",
    "    \n",
    "run_examples(EXAMPLES1)\n",
    "print('---------')\n",
    "run_examples(EXAMPLES2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be some commonalities to the mistakes that the model makes:\n",
    "* Mistakes are more common when the human-readable input text is longer\n",
    "* Mistakes are more common when information (such as the day or the year) have different positions in the sentence relative to each other.\n",
    "\n",
    "We'll now try to get some generalizations for these intuitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Input Length on Model Accuracy\n",
    "\n",
    "For each instance of our training data, we'll perform the following steps:\n",
    "* Calculate the length of the human-readable form\n",
    "* Calculate whether the *entire* sequence is correct or not - that is, a single wrong digit makes the entire sequence wrong\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Source Length</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Is Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05.06.94</td>\n",
       "      <td>1994-06-05</td>\n",
       "      <td>8</td>\n",
       "      <td>1994-06-04</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 JULY, 1981</td>\n",
       "      <td>1981-07-13</td>\n",
       "      <td>13</td>\n",
       "      <td>1971-08-11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 october, 1995</td>\n",
       "      <td>1995-10-12</td>\n",
       "      <td>16</td>\n",
       "      <td>1993-10-22</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06.12.16</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 jan, 1972</td>\n",
       "      <td>1972-01-10</td>\n",
       "      <td>12</td>\n",
       "      <td>1972-01-10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Source      Target  Source Length      Actual  Is Correct\n",
       "0          05.06.94  1994-06-05              8  1994-06-04       False\n",
       "1     13 JULY, 1981  1981-07-13             13  1971-08-11       False\n",
       "2  12 october, 1995  1995-10-12             16  1993-10-22       False\n",
       "3          06.12.16  2016-12-06              8  2016-12-16       False\n",
       "4      10 jan, 1972  1972-01-10             12  1972-01-10        True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all data into a single data frame for convenience\n",
    "df = pd.DataFrame(data=dataset[:8000], columns=['Source','Target'])\n",
    "df['Source Length'] = df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "# Have the model predict all training data\n",
    "model_predictions = model.predict(sources[:8000])\n",
    "df['Actual'] = [prediction_to_text([x]) for x in model_predictions]\n",
    "\n",
    "# Check if prediction matches ground truth\n",
    "df['Is Correct'] = df['Actual'] == df['Target']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now group by the source length, and plot the percent of model predictions that matched the ground truth targets.  As discussed, we plot the accuracy for an *entire* prediction, not just a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not df['Is Correct'].any():\n",
    "    print(\"Please re-train the model!\")\n",
    "else:\n",
    "    ax = pd.crosstab(df['Source Length'], df['Is Correct'], normalize='index')[True].plot(kind='line', figsize=(20,7))\n",
    "    ax.set_ylabel(\"% Correct\")\n",
    "    ax.set_xlabel(\"Input Length (characters)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would indeed appear that the model has difficulties when the source dates in human-readable format are longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the Location of the Day on Model Accuracy\n",
    "\n",
    "For each training instance, let's locate the day - i.e., the *3* in *July 3* in the machine-readable format.  This is easy, as we defined this format to be in a standard form.  We'll then locate that day in the human-readable format and try to determine whether this location has any bearing on the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_location = 8 #  Recall that the machine-readable format is YYYY-MM-DD.  We look for the index where DD begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Source Length</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Is Correct</th>\n",
       "      <th>Day Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05.06.94</td>\n",
       "      <td>1994-06-05</td>\n",
       "      <td>8</td>\n",
       "      <td>1994-06-04</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 JULY, 1981</td>\n",
       "      <td>1981-07-13</td>\n",
       "      <td>13</td>\n",
       "      <td>1971-08-11</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12 october, 1995</td>\n",
       "      <td>1995-10-12</td>\n",
       "      <td>16</td>\n",
       "      <td>1993-10-22</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06.12.16</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>8</td>\n",
       "      <td>2016-12-16</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 jan, 1972</td>\n",
       "      <td>1972-01-10</td>\n",
       "      <td>12</td>\n",
       "      <td>1972-01-10</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Source      Target  Source Length      Actual  Is Correct  \\\n",
       "0          05.06.94  1994-06-05              8  1994-06-04       False   \n",
       "1     13 JULY, 1981  1981-07-13             13  1971-08-11       False   \n",
       "2  12 october, 1995  1995-10-12             16  1993-10-22       False   \n",
       "3          06.12.16  2016-12-06              8  2016-12-16       False   \n",
       "4      10 jan, 1972  1972-01-10             12  1972-01-10        True   \n",
       "\n",
       "   Day Location  \n",
       "0             1  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_data = []\n",
    "for x in df.iterrows():\n",
    "    source = x[1].Source\n",
    "    day_value = x[1].Target[day_location:day_location+2]\n",
    "    if day_value[0] == '0':\n",
    "        day_value = day_value[1]\n",
    "    day_index = source.find(day_value)\n",
    "        \n",
    "    location_data.append((day_index))\n",
    "    \n",
    "location_data = np.array(location_data)\n",
    "\n",
    "df['Day Location'] = location_data\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not df['Is Correct'].any():\n",
    "    print(\"Please re-train the model!\")\n",
    "else:\n",
    "    ax = pd.crosstab(df['Day Location'], df['Is Correct'], normalize='index')[True].plot(kind='line', figsize=(20,7))\n",
    "    ax.set_ylabel(\"% Correct\")\n",
    "    ax.set_xlabel(\"Location of Day Component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here too it would appear that the model has difficulties the further away the day component is from the beginning of the input. \n",
    "\n",
    "Naturally, an interesting question to ask is how correlated the day component location and input length are.  Are we actually looking at the same problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation coefficient:  0.7562277395175759\n"
     ]
    }
   ],
   "source": [
    "# Generate the Spearman correlation coefficient between the input source length and location of the day component.\n",
    "print(\"Spearman correlation coefficient: \", np.corrcoef(df['Source Length'], df['Day Location'])[0][1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a fair bit of correlation, but the two factors are not identical.  We can therefore conclude that the model does best on 'easy' cases where it knows precisely where every part of the human-readable date is.  If we add in some form of 'noise' - for example, moving the day or year, using a full month name or adding superfluous components such as week day names - the model gets 'confused'.  Ideally, we would like a mechanism whereby the model can *focus* or *concentrate*.  In other words - the model will recognize the day, month and year no matter where they appear in the input and ignore parts of the input that do not affect the results.\n",
    "\n",
    "Will will now see how to do this using the concept of *attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Results Using Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explain the concept of attention, we'll try a little exercise.  Look around you and pick some object like a light switch or a chair.  Look at the object for a few seconds and focus on it completely.  Notice how this object stays sharp in your field of view, while other objects seem to just fade into the background.  Also, note that this fading is gradual - things that are closer to your object are sharper while things further away are not.\n",
    "\n",
    "This, in a nutshell, is attention - the ability to concentrate on what's relevant and ignore that which is irrelevant for the task at hand.  In Deep Learning, attention is a technique which is used to help the model focus - at every timestep - on only the relevant inputs.  For our purposes, it is the ability to focus on specific tokens while ignoring the rest.  To be more precise, we'll be using *soft-attention* - that is, we will not ignore tokens completely but rather assign them *weights* that determine how much the model should focus on them at any moment in time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/NMT_Components.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 3**: Attention NMT model</center></caption>\n",
    "\n",
    "As we've seen, we input the human-readable date to the encoder's Bi-LSTM network and take the resulting state vectors as the input to the decoder.  What we would like to do is - for every character in the output (that is for every timestep of the decoder) - have the model focus on the state vector that is most helpful *right now*.  Adding an attention block between the encoder and decoder does just that.\n",
    "\n",
    "<img src=\"images/Attention_mechanism.png\" style=\"width:600;height:300px;\"> <br>\n",
    "<caption><center> **Figure 4**: Attention Implementation</center>\n",
    "    <center><i><a href='https://arxiv.org/abs/1409.0473'>Source: Dmzimtry Bahdanau, Kyung Hyun Cho and Yoshua Bengio, Neural Machine Translation by Jointly Learning to Align and Translate</a> </i> </center>\n",
    "</caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure we can see the details of the attention block.  Each hidden state $h_i$ is composed of the combined output of the forwards and backwards LSTM (i.e., a Bi-LSTM). Furthermore, each such state has an associated weight at each timestep (the $\\alpha_t$ values). This weighted sum of the states, along with the hidden state of the decoder network at time $t-1$, along with the output of the decoder at $t-1$, are used to calculate the output of the decoder at time $t$.  $\\alpha_t$'s are calculated using an *alignment model*, which learns which encoder state ($h_j$) is relevant at what point in the decoder.\n",
    "\n",
    "\n",
    "Attention is a general mechanism - the above is one way of implementing this 'focusing' behavior, known as *Bahdanau Attention*.  Another popular implementation is described [here](https://arxiv.org/abs/1508.04025) and is known as *Luong Attention*.  \n",
    "\n",
    "While the concept of attention is not difficult to describe, its technical implementation is not trivial.  Rather than implementing it ourselves, we'll use ready-made implementations that are freely available in NVIDIA's open-source Seq2Seq toolkit, appropriately named Open Seq2Seq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Open Seq2Seq to the Dates Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open Seq2Seq](https://github.com/NVIDIA/OpenSeq2Seq) is a Tensorflow-based toolkit for sequence-to-sequence problems that use the encoder-decoder architecture.  It contains models for machine translation, as well as speech-to-text, text-to-speech and image captioning.  As part of these models, Open Seq2Seq provides generic implementation for various types of encoders, decoders, loss functions and attention mechanism implementations.  \n",
    "\n",
    "In order to work with Open Seq2Seq, we need the following files:\n",
    "* Source sequences - Each line in this file contains a source sequence to be used as input.  In our case, this is the human-readable date.  \n",
    "* Target sequences - Each line in this file contains a target sequence to be used as the required output.  In our case, this is the machine-readable date.  \n",
    "* Source vocabulary - The individual tokens that make up the source sequences.  In our case - since we are 'translating' individual characters - these will be individual letters, numbers and symbols.  In more general cases, this vocabulary will contain individual words.  Note that this vocabulary also contains several special tokens for marking padding, beginning and end-of-sentence tokens.\n",
    "* Target vocabulary - The individual tokens that make up the target sequences.  In our case, these will be numbers and the dash symbol, along with the special tokens described above.\n",
    "\n",
    "The following code generates all of the required files in the `dates` directory.  We create source and target files for both a training and a test set.  An interesting implementation detail is the way we split our dates into individual characters:  since the whitespace character is a valid token in the human-readable date, we use the pipe symbol (|) as the delimiter between characters.\n",
    "\n",
    "Run the following code, and inspect the files that are generated in the `dates` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('./dates'):\n",
    "    os.makedirs('./dates')\n",
    "\n",
    "\n",
    "def save_data(filename, data):\n",
    "    with open('dates/' + filename + '.txt', 'w') as f:\n",
    "        for x in data:\n",
    "            f.write(x)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "save_data('source_train', map(lambda x: '|'.join(x[0]), dataset[:8000]))\n",
    "save_data('target_train', map(lambda x: '|'.join(x[1]), dataset[:8000]))\n",
    "save_data('source_test', map(lambda x: '|'.join(x[0]), dataset[8000:]))\n",
    "save_data('target_test', map(lambda x: '|'.join(x[1]), dataset[8000:]))\n",
    "save_data('source_vocab', [key for key,value in sorted(human_vocab.items(), key=lambda x: x[1])])\n",
    "save_data('target_vocab', [key for key,value in sorted(machine_vocab.items(), key=lambda x: x[1])])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of its distribution, Open Seq2Seq provides a generic execution engine that can be configured for the appropriate use-case.  Open the `dates_config.py` - you can see that the configuration is in fact a set of python dictionaries that define model type and architecture.\n",
    "\n",
    "In particular, note the following sections in the configuration:\n",
    "\n",
    "*  `base_params` - This section defines parameters such as the number of GPUs to use for training, the number of epochs, the optimizer and learning rate and when to print statistics and/or save temporary models.\n",
    "*  `encoder` - This section defines the encoder part of the architecture.  In our case, we use an encoder that contains a Bi-LSTM with Embedding, with the layer count as well as the LSTM and embedding sizes that match our hand-coded model from above.\n",
    "*  `decoder` - This section defines the decoder part of the architecture.  In our case, we use a decoder that contains a regular LSTM with Bahdanau attention.  In addition to the number of layers and LSTM and embedding sizes, you can also the definition of the special tokens we described in the vocabulary section above.  \n",
    "*  `train_params` - This section defines the source of the training data.  Here you can find the source and target sequences as well as the vocabulary files and the delimiter definition.\n",
    "* `infer_params` - This section defines the model used during inference, as opposed to training.  Note that we use a different decoder during inference - rather than just an LSTM and attention, we also apply Beam Search (see below) to find the best sequence. Other than that, the changes from the previous sections are around the use of different source and target sequence files.\n",
    "\n",
    "For completeness, it should also be mentioned that there is also an `eval_params` section that can be used to perform evaluation during and/or after training.  We will use it a little later on.\n",
    "\n",
    "Run the following code to train the model using attention.  Notice the train loss values, as well as the sample source, target and prediction texts that the model displays as it's running.  We are using the same parameters as before, but now the model should rapidly converge.  \n",
    "\n",
    "(Note:  if you'd like to run this multiple times, uncomment the following cell and run it so that it deletes the `dates_log` directory before each training run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./dates_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Starting training from scratch\n",
      "*** Training config:\n",
      "{'batch_size_per_gpu': 128,\n",
      " 'data_layer': <class 'open_seq2seq2.data.text2text.text2text.ParallelTextDataLayer'>,\n",
      " 'data_layer_params': {'delimiter': '|',\n",
      "                       'map_parallel_calls': 16,\n",
      "                       'max_length': 60,\n",
      "                       'prefetch_buffer_size': 2,\n",
      "                       'repeat': True,\n",
      "                       'shuffle': True,\n",
      "                       'source_file': 'dates/source_train.txt',\n",
      "                       'src_vocab_file': 'dates/source_vocab.txt',\n",
      "                       'target_file': 'dates/target_train.txt',\n",
      "                       'tgt_vocab_file': 'dates/target_vocab.txt'},\n",
      " 'decoder': <class 'open_seq2seq2.decoders.rnn_decoders.RNNDecoderWithAttention'>,\n",
      " 'decoder_params': {'END_SYMBOL': 1,\n",
      "                    'GO_SYMBOL': 2,\n",
      "                    'PAD_SYMBOL': 0,\n",
      "                    'attention_layer_size': 128,\n",
      "                    'attention_type': 'bahdanau',\n",
      "                    'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'decoder_dp_input_keep_prob': 0.8,\n",
      "                    'decoder_dp_output_keep_prob': 1.0,\n",
      "                    'decoder_layers': 1,\n",
      "                    'decoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'tgt_emb_size': 64},\n",
      " 'dtype': tf.float32,\n",
      " 'encoder': <class 'open_seq2seq2.encoders.rnn_encoders.BidirectionalRNNEncoderWithEmbedding'>,\n",
      " 'encoder_params': {'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'encoder_dp_input_keep_prob': 0.8,\n",
      "                    'encoder_dp_output_keep_prob': 1.0,\n",
      "                    'encoder_layers': 1,\n",
      "                    'encoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'src_emb_size': 64},\n",
      " 'load_model': '',\n",
      " 'logdir': 'dates_log',\n",
      " 'loss': <class 'open_seq2seq2.losses.sequence_loss.BasicSequenceLoss'>,\n",
      " 'loss_params': {'average_across_timestep': False,\n",
      "                 'do_mask': True,\n",
      "                 'offset_target_by_one': True},\n",
      " 'lr_policy': <function fixed_lr at 0x7fd4786dc620>,\n",
      " 'lr_policy_params': {'learning_rate': 0.001},\n",
      " 'num_epochs': 40,\n",
      " 'num_gpus': 1,\n",
      " 'optimizer': 'RMSProp',\n",
      " 'optimizer_params': {},\n",
      " 'print_loss_steps': 500,\n",
      " 'print_samples_steps': 500,\n",
      " 'save_checkpoint_steps': 2500,\n",
      " 'use_horovod': False}\n",
      "*** Building graph on GPU:0\n",
      "*** Trainable variables:\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/EncoderEmbeddingMatrix:0\n",
      "***     shape: (62, 64), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "***     shape: (96, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "***     shape: (96, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/DecoderEmbeddingMatrix:0\n",
      "***     shape: (15, 64), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/AttentionMechanism/memory_layer/kernel:0\n",
      "***     shape: (64, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "***     shape: (160, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel:0\n",
      "***     shape: (32, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/bahdanau_attention/attention_v:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/dense/kernel:0\n",
      "***     shape: (64, 15), <dtype: 'float32_ref'>\n",
      "*** Total trainable parameters: 63744\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into dates_log/model.ckpt.\n",
      "*** Epoch 0, global step 0: ***     Train loss: 29.6988 \n",
      "time per step = 0:00:0.009\n",
      "***     Train Source[0]:     <s>|1|1|.|1|2|.|8|2|<eos>\n",
      "***     Train Target[0]:     <s>|1|9|8|2|-|1|2|-|1|1|<eos>\n",
      "***     Train Prediction[0]: 2|2|2|2|2|2|2|2|2|2|2|2\n",
      "*** Epoch 8, global step 500: ***     Train loss: 10.2279 \n",
      "time per step = 0:00:0.094\n",
      "***     Train Source[0]:     <s>|2|1|<unk>|1|2|<unk>|1|3|<eos>\n",
      "***     Train Target[0]:     <s>|2|0|1|3|-|1|2|-|2|1|<eos>\n",
      "***     Train Prediction[0]: 2|1|1|1|-|2|1|-|2|1|-|-\n",
      "*** Epoch 16, global step 1000: ***     Train loss: 1.5200 \n",
      "time per step = 0:00:0.091\n",
      "***     Train Source[0]:     <s>|m|a|r|c|h|<unk>|3|0|,|<unk>|1|9|8|8|<eos>\n",
      "***     Train Target[0]:     <s>|1|9|8|8|-|0|3|-|3|0|<eos>\n",
      "***     Train Prediction[0]: 1|9|8|8|-|0|3|-|3|0|<eos>|3\n",
      "*** Epoch 24, global step 1500: ***     Train loss: 0.0495 \n",
      "time per step = 0:00:0.091\n",
      "***     Train Source[0]:     <s>|J|A|N|<unk>|4|,|<unk>|2|0|1|8|<eos>\n",
      "***     Train Target[0]:     <s>|2|0|1|8|-|0|1|-|0|4|<eos>\n",
      "***     Train Prediction[0]: 2|0|1|8|-|0|1|-|0|4|<eos>|0\n",
      "*** Epoch 32, global step 2000: ***     Train loss: 0.0038 \n",
      "time per step = 0:00:0.091\n",
      "***     Train Source[0]:     <s>|2|2|<unk>|0|7|<unk>|8|7|<eos>\n",
      "***     Train Target[0]:     <s>|1|9|8|7|-|0|7|-|2|2|<eos>\n",
      "***     Train Prediction[0]: 1|9|8|7|-|0|7|-|2|2|<eos>|2\n",
      "INFO:tensorflow:Saving checkpoints for 2480 into dates_log/model.ckpt.\n",
      "*** Finished training\n",
      "*** Avg time per step: 0.091s\n",
      "*** Avg objects per second: 37322.328\n",
      "CPU times: user 8min, sys: 37.6 s, total: 8min 38s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_open_seq2seq(\n",
    "    new_run, \n",
    "    '--config_file=dates_config.py --mode=train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the model using 80% of the data in our original toy dataset.  We will now run inference on the remaining 20% and apply the same metrics as before - namely, whether the sequences are correct or not and how correctness varies with input length. \n",
    "\n",
    "In order to perform inference we use the same configuration as before, setting the `mode` parameter to *infer* rather than *train*.  The output of inference is written to a file named `infer-out.txt`.  Run the following three cells in order and compare the resulting plot to the one generated by the hand-coded model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Restoring from the latest checkpoint\n",
      "*** Loading model from dates_log/model.ckpt-2480\n",
      "*** Inference config:\n",
      "{'batch_size_per_gpu': 1,\n",
      " 'data_layer': <class 'open_seq2seq2.data.text2text.text2text.ParallelTextDataLayer'>,\n",
      " 'data_layer_params': {'delimiter': '|',\n",
      "                       'map_parallel_calls': 16,\n",
      "                       'max_length': 60,\n",
      "                       'prefetch_buffer_size': 2,\n",
      "                       'repeat': False,\n",
      "                       'shuffle': False,\n",
      "                       'source_file': 'dates/source_test.txt',\n",
      "                       'src_vocab_file': 'dates/source_vocab.txt',\n",
      "                       'target_file': 'dates/source_test.txt',\n",
      "                       'tgt_vocab_file': 'dates/target_vocab.txt'},\n",
      " 'decoder': <class 'open_seq2seq2.decoders.rnn_decoders.BeamSearchRNNDecoderWithAttention'>,\n",
      " 'decoder_params': {'END_SYMBOL': 1,\n",
      "                    'GO_SYMBOL': 2,\n",
      "                    'PAD_SYMBOL': 0,\n",
      "                    'attention_layer_size': 128,\n",
      "                    'attention_type': 'bahdanau',\n",
      "                    'beam_width': 1,\n",
      "                    'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'decoder_dp_input_keep_prob': 0.8,\n",
      "                    'decoder_dp_output_keep_prob': 1.0,\n",
      "                    'decoder_layers': 1,\n",
      "                    'decoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'length_penalty': 0.0,\n",
      "                    'tgt_emb_size': 64},\n",
      " 'dtype': tf.float32,\n",
      " 'encoder': <class 'open_seq2seq2.encoders.rnn_encoders.BidirectionalRNNEncoderWithEmbedding'>,\n",
      " 'encoder_params': {'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'encoder_dp_input_keep_prob': 0.8,\n",
      "                    'encoder_dp_output_keep_prob': 1.0,\n",
      "                    'encoder_layers': 1,\n",
      "                    'encoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'src_emb_size': 64},\n",
      " 'load_model': '',\n",
      " 'logdir': 'dates_log',\n",
      " 'loss': <class 'open_seq2seq2.losses.sequence_loss.BasicSequenceLoss'>,\n",
      " 'loss_params': {'average_across_timestep': False,\n",
      "                 'do_mask': True,\n",
      "                 'offset_target_by_one': True},\n",
      " 'lr_policy': <function fixed_lr at 0x7fd4786dc620>,\n",
      " 'lr_policy_params': {'learning_rate': 0.001},\n",
      " 'num_epochs': 40,\n",
      " 'num_gpus': 1,\n",
      " 'optimizer': 'RMSProp',\n",
      " 'optimizer_params': {},\n",
      " 'print_loss_steps': 500,\n",
      " 'print_samples_steps': 500,\n",
      " 'save_checkpoint_steps': 2500,\n",
      " 'use_horovod': False}\n",
      "*** Building graph on GPU:0\n",
      "*** Inference Mode. Loss part of graph isn't built.\n",
      "INFO:tensorflow:Restoring parameters from dates_log/model.ckpt-2480\n",
      "*** Processed 1/2000 batches\n",
      "*** Processed 200/2000 batches\n",
      "*** Processed 400/2000 batches\n",
      "*** Processed 600/2000 batches\n",
      "*** Processed 800/2000 batches\n",
      "*** Processed 1000/2000 batches\n",
      "*** Processed 1200/2000 batches\n",
      "*** Processed 1400/2000 batches\n",
      "*** Processed 1600/2000 batches\n",
      "*** Processed 1800/2000 batches\n",
      "*** Processed 2000/2000 batches\n",
      "*** Processed 2000/2000 batches\n",
      "*** Avg time per step: 0.0357s\n",
      "*** Avg objects per second: 747.378\n",
      "*** Input sequence:  3 1 . 1 0 . 0 5\n",
      "*** Output sequence: 2 0 0 5 - 1 0 - 3 1\n",
      "*** \n",
      "*** Input sequence:  2 1 <unk> 0 8 <unk> 7 2\n",
      "*** Output sequence: 1 9 7 2 - 0 8 - 2 1\n",
      "*** \n",
      "*** Input sequence:  M a r c h <unk> 2 1 , <unk> 1 9 9 7\n",
      "*** Output sequence: 1 9 9 7 - 0 3 - 2 1\n",
      "*** \n",
      "*** Input sequence:  2 5 <unk> 1 0 <unk> 8 5\n",
      "*** Output sequence: 1 9 8 5 - 1 0 - 2 5\n",
      "*** \n",
      "*** Input sequence:  1 7 <unk> N O V <unk> 2 0 0 3\n",
      "*** Output sequence: 2 0 0 3 - 1 1 - 1 7\n",
      "*** \n",
      "*** Input sequence:  D E C E M B E R <unk> 1 8 <unk> 2 0 0 6\n",
      "*** Output sequence: 2 0 0 6 - 1 2 - 1 8\n",
      "*** \n",
      "*** Input sequence:  J u n e <unk> 9 , <unk> 2 0 1 9\n",
      "*** Output sequence: 2 0 1 9 - 0 6 - 0 9\n",
      "*** \n",
      "*** Input sequence:  T h u r s d a y , <unk> F e b r u a r y <unk> 4 , <unk> 2 0 1 6\n",
      "*** Output sequence: 2 0 1 6 - 0 2 - 0 4\n",
      "*** \n",
      "*** Input sequence:  6 <unk> J A N U A R Y <unk> 1 9 9 1\n",
      "*** Output sequence: 1 9 9 1 - 0 1 - 0 6\n",
      "*** \n",
      "*** Input sequence:  J U L Y <unk> 2 9 <unk> 1 9 7 9\n",
      "*** Output sequence: 1 9 7 9 - 0 7 - 2 9\n",
      "*** \n",
      "*** Finished inference\n",
      "CPU times: user 3min 8s, sys: 21.6 s, total: 3min 30s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_open_seq2seq(\n",
    "    new_run, \n",
    "    '--config_file=dates_config.py --mode=infer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Is Correct</th>\n",
       "      <th>Source Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.10.05</td>\n",
       "      <td>2005-10-31</td>\n",
       "      <td>2005-10-31</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 Jul 1981</td>\n",
       "      <td>1981-07-04</td>\n",
       "      <td>1981-07-04</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>may 15, 1982</td>\n",
       "      <td>1982-05-15</td>\n",
       "      <td>1982-05-15</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wednesday, July 3, 2019</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>2019-07-03</td>\n",
       "      <td>True</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9 April, 2002</td>\n",
       "      <td>2002-04-09</td>\n",
       "      <td>2002-04-09</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Source      Target      Actual  Is Correct  Source Length\n",
       "0                 31.10.05  2005-10-31  2005-10-31        True              8\n",
       "1               4 Jul 1981  1981-07-04  1981-07-04        True             10\n",
       "2             may 15, 1982  1982-05-15  1982-05-15        True             12\n",
       "3  Wednesday, July 3, 2019  2019-07-03  2019-07-03        True             23\n",
       "4            9 April, 2002  2002-04-09  2002-04-09        True             13"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    pd.DataFrame(dataset[8000:], columns=['Source', 'Target']), \n",
    "    pd.read_csv('infer-out.txt', names=['Actual'], header=None) \n",
    "], axis=1)\n",
    "df['Actual'] = df['Actual'].str.replace(' ','')\n",
    "df['Is Correct'] = df['Actual'] == df['Target']\n",
    "df['Source Length'] = df['Source'].apply(lambda x: len(x))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy: {}\".format(df['Is Correct'].sum() / len(df)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not df['Is Correct'].any():\n",
    "    print(\"Please re-train the model!\")\n",
    "else:\n",
    "    ax = pd.crosstab(df['Source Length'], df['Is Correct'], normalize='index')[True].plot(kind='line', figsize=(20,7))\n",
    "    ax.set_ylabel(\"% Correct\")\n",
    "    ax.set_xlabel(\"Input Length (characters)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Better Score Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, we use a character-by-character comparison to measure the date translation problem's accuracy.  For this problem, such a measure of success is sensible, as there is only one possible output 'translation'.  When dealing with real-world machine translation tasks, however, measuring results in this way is not a good idea.  To illustrate why, let's consider a slight modification on our date translation problem.\n",
    "\n",
    "In our problem statement, we wish to translate from a human-readable format to a *specific* machine readable format, namely YYYY-MM-dd.  Similarly, we could have used a different format, such as dd-MM-YYYY.  So for a date such as 'January 12, 1984' we could have several translations.  For example:\n",
    "\n",
    "* 1984-01-12 (YYYY-MM-dd)\n",
    "* 12-01-1984 (dd-MM-YYYY)\n",
    "* 1984-12-01 (YYYY-dd-MM)\n",
    "\n",
    "or any other such variant.  Unless stated explicitly, all machine-readable formats can be considered to be equally good translations.  Naturally, this applies even more strongly to human language: different people may translate the same source-language sentence to multiple variants in the target language.  Can we distinctly say which one is 'better' or 'worse'?  For example, here are some English translations of a Chinese sentence (taken from the original BLEU paper - see below):\n",
    "\n",
    "* It is a guide to action that ensures that the military will forever heed Party commands.\n",
    "* It is the guiding principle which guarantees the military forces always being under the command of the Party.\n",
    "* It is the practical guide for the army always to heed the directions of the party.\n",
    "\n",
    "We can see that a better metric is needed for measuring machine translation results than just plain 'is this character in the right spot'.  This metric is known as the [Bilingual Evaluation Understudy](https://en.wikipedia.org/wiki/BLEU), or BLEU, score.  We will implement BLEU below.  For now, let's get some intuition on the BLEU scores for our toy problem.  In the process, we'll also get to see some more features of Open Seq2Seq.\n",
    "\n",
    "__Exercise__:  Open the file `dates_eval_config.py` and complete the missing pieces in the `eval_params` section.  Use the *test* versions of the source and target files. Run the following cells to re-train the model and observe the BLEU scores during training.  A completed file is available in `dates_eval_config.py.solution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./dates_eval_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Starting training from scratch\n",
      "*** Training config:\n",
      "{'batch_size_per_gpu': 128,\n",
      " 'data_layer': <class 'open_seq2seq2.data.text2text.text2text.ParallelTextDataLayer'>,\n",
      " 'data_layer_params': {'delimiter': '|',\n",
      "                       'map_parallel_calls': 16,\n",
      "                       'max_length': 60,\n",
      "                       'prefetch_buffer_size': 2,\n",
      "                       'repeat': True,\n",
      "                       'shuffle': True,\n",
      "                       'source_file': 'dates/source_train.txt',\n",
      "                       'src_vocab_file': 'dates/source_vocab.txt',\n",
      "                       'target_file': 'dates/target_train.txt',\n",
      "                       'tgt_vocab_file': 'dates/target_vocab.txt'},\n",
      " 'decoder': <class 'open_seq2seq2.decoders.rnn_decoders.RNNDecoderWithAttention'>,\n",
      " 'decoder_params': {'END_SYMBOL': 1,\n",
      "                    'GO_SYMBOL': 2,\n",
      "                    'PAD_SYMBOL': 0,\n",
      "                    'attention_layer_size': 128,\n",
      "                    'attention_type': 'bahdanau',\n",
      "                    'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'decoder_dp_input_keep_prob': 0.8,\n",
      "                    'decoder_dp_output_keep_prob': 1.0,\n",
      "                    'decoder_layers': 1,\n",
      "                    'decoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'tgt_emb_size': 64},\n",
      " 'dtype': tf.float32,\n",
      " 'encoder': <class 'open_seq2seq2.encoders.rnn_encoders.BidirectionalRNNEncoderWithEmbedding'>,\n",
      " 'encoder_params': {'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'encoder_dp_input_keep_prob': 0.8,\n",
      "                    'encoder_dp_output_keep_prob': 1.0,\n",
      "                    'encoder_layers': 1,\n",
      "                    'encoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'src_emb_size': 64},\n",
      " 'eval_steps': 500,\n",
      " 'load_model': '',\n",
      " 'logdir': 'dates_eval_log',\n",
      " 'loss': <class 'open_seq2seq2.losses.sequence_loss.BasicSequenceLoss'>,\n",
      " 'loss_params': {'average_across_timestep': False,\n",
      "                 'do_mask': True,\n",
      "                 'offset_target_by_one': True},\n",
      " 'lr_policy': <function fixed_lr at 0x7fd4786dc620>,\n",
      " 'lr_policy_params': {'learning_rate': 0.001},\n",
      " 'num_epochs': 40,\n",
      " 'num_gpus': 1,\n",
      " 'optimizer': 'RMSProp',\n",
      " 'optimizer_params': {},\n",
      " 'print_loss_steps': 500,\n",
      " 'print_samples_steps': 500,\n",
      " 'save_checkpoint_steps': 2500,\n",
      " 'use_horovod': False}\n",
      "*** Evaluation config:\n",
      "{'batch_size_per_gpu': 128,\n",
      " 'data_layer': <class 'open_seq2seq2.data.text2text.text2text.ParallelTextDataLayer'>,\n",
      " 'data_layer_params': {'delimiter': '|',\n",
      "                       'map_parallel_calls': 5,\n",
      "                       'max_length': 60,\n",
      "                       'prefetch_buffer_size': 2,\n",
      "                       'repeat': True,\n",
      "                       'shuffle': False,\n",
      "                       'source_file': 'dates/source_test.txt',\n",
      "                       'src_vocab_file': 'dates/source_vocab.txt',\n",
      "                       'target_file': 'dates/target_test.txt',\n",
      "                       'tgt_vocab_file': 'dates/target_vocab.txt'},\n",
      " 'decoder': <class 'open_seq2seq2.decoders.rnn_decoders.RNNDecoderWithAttention'>,\n",
      " 'decoder_params': {'END_SYMBOL': 1,\n",
      "                    'GO_SYMBOL': 2,\n",
      "                    'PAD_SYMBOL': 0,\n",
      "                    'attention_layer_size': 128,\n",
      "                    'attention_type': 'bahdanau',\n",
      "                    'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'decoder_dp_input_keep_prob': 0.8,\n",
      "                    'decoder_dp_output_keep_prob': 1.0,\n",
      "                    'decoder_layers': 1,\n",
      "                    'decoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'tgt_emb_size': 64},\n",
      " 'dtype': tf.float32,\n",
      " 'encoder': <class 'open_seq2seq2.encoders.rnn_encoders.BidirectionalRNNEncoderWithEmbedding'>,\n",
      " 'encoder_params': {'core_cell': <class 'tensorflow.python.ops.rnn_cell_impl.LSTMCell'>,\n",
      "                    'core_cell_params': {'forget_bias': 1.0, 'num_units': 32},\n",
      "                    'encoder_dp_input_keep_prob': 0.8,\n",
      "                    'encoder_dp_output_keep_prob': 1.0,\n",
      "                    'encoder_layers': 1,\n",
      "                    'encoder_use_skip_connections': False,\n",
      "                    'initializer': <function glorot_uniform_initializer at 0x7fd517232950>,\n",
      "                    'src_emb_size': 64},\n",
      " 'eval_steps': 500,\n",
      " 'load_model': '',\n",
      " 'logdir': 'dates_eval_log',\n",
      " 'loss': <class 'open_seq2seq2.losses.sequence_loss.BasicSequenceLoss'>,\n",
      " 'loss_params': {'average_across_timestep': False,\n",
      "                 'do_mask': True,\n",
      "                 'offset_target_by_one': True},\n",
      " 'lr_policy': <function fixed_lr at 0x7fd4786dc620>,\n",
      " 'lr_policy_params': {'learning_rate': 0.001},\n",
      " 'num_epochs': 40,\n",
      " 'num_gpus': 1,\n",
      " 'optimizer': 'RMSProp',\n",
      " 'optimizer_params': {},\n",
      " 'print_loss_steps': 500,\n",
      " 'print_samples_steps': 500,\n",
      " 'save_checkpoint_steps': 2500,\n",
      " 'use_horovod': False}\n",
      "*** Building graph on GPU:0\n",
      "*** Trainable variables:\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/EncoderEmbeddingMatrix:0\n",
      "***     shape: (62, 64), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "***     shape: (96, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/fw/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "***     shape: (96, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/bidir_rnn_encoder_with_emb/bidirectional_rnn/bw/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/DecoderEmbeddingMatrix:0\n",
      "***     shape: (15, 64), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/AttentionMechanism/memory_layer/kernel:0\n",
      "***     shape: (64, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/kernel:0\n",
      "***     shape: (160, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/multi_rnn_cell/cell_0/lstm_cell/bias:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/bahdanau_attention/query_layer/kernel:0\n",
      "***     shape: (32, 128), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/attention_wrapper/bahdanau_attention/attention_v:0\n",
      "***     shape: (128,), <dtype: 'float32_ref'>\n",
      "***   ForwardPass/rnn_decoder_with_attention/decoder/dense/kernel:0\n",
      "***     shape: (64, 15), <dtype: 'float32_ref'>\n",
      "*** Total trainable parameters: 63744\n",
      "*** Building graph on GPU:0\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into dates_eval_log/model.ckpt.\n",
      "*** Running evaluation on a validation set:\n",
      "***     *****EVAL Source[0]:     <s>|3|1|.|1|0|.|0|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|M|A|Y|,|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1\n",
      "***     *****EVAL Source[0]:     <s>|J|a|n|u|a|r|y|<unk>|2|3|,|<unk>|1|9|7|8|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|1|0|<unk>|j|u|l|y|<unk>|2|0|0|1|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***     *****EVAL Source[0]:     <s>|A|U|G|U|S|T|<unk>|2|7|<unk>|1|9|9|0|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|5|<unk>|2|0|1|7|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|1|8|<unk>|J|U|N|E|<unk>|1|9|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|0|7|.|0|2|.|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1\n",
      "***     *****EVAL Source[0]:     <s>|M|A|Y|<unk>|1|8|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1|1\n",
      "***     *****EVAL Source[0]:     <s>|2|5|,|<unk>|o|c|t|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|N|O|V|E|M|B|E|R|<unk>|1|<unk>|1|9|8|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|1|9|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|D|E|C|E|M|B|E|R|<unk>|1|9|9|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|1|9|<unk>|S|E|P|T|E|M|B|E|R|<unk>|2|0|1|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|a|p|r|i|l|<unk>|2|,|<unk>|1|9|8|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     *****EVAL Source[0]:     <s>|T|u|e|s|d|a|y|,|<unk>|O|c|t|o|b|e|r|<unk>|2|6|,|<unk>|1|9|9|9|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     *****EVAL Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8|8\n",
      "***     Validation loss: 29.7336 \n",
      "***     Eval BLUE score: 0.0\n",
      "*** Epoch 0, global step 0: ***     Train loss: 29.7604 \n",
      "time per step = 0:00:0.018\n",
      "***     Train Source[0]:     <s>|S|A|T|U|R|D|A|Y|,|<unk>|A|U|G|U|S|T|<unk>|1|2|,|<unk>|1|9|8|9|<eos>\n",
      "***     Train Target[0]:     <s>|1|9|8|9|-|0|8|-|1|2|<eos>\n",
      "***     Train Prediction[0]: 8|8|8|8|8|8|8|8|8|8|8|8\n",
      "*** Running evaluation on a validation set:\n",
      "***     *****EVAL Source[0]:     <s>|3|1|.|1|0|.|0|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|0|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0|1|-|0\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|M|A|Y|,|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0\n",
      "***     *****EVAL Source[0]:     <s>|J|a|n|u|a|r|y|<unk>|2|3|,|<unk>|1|9|7|8|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0\n",
      "***     *****EVAL Source[0]:     <s>|1|0|<unk>|j|u|l|y|<unk>|2|0|0|1|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|0|0|0|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7\n",
      "***     *****EVAL Source[0]:     <s>|A|U|G|U|S|T|<unk>|2|7|<unk>|1|9|9|0|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|5|<unk>|2|0|1|7|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|0|1|7|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-|0|0|-\n",
      "***     *****EVAL Source[0]:     <s>|1|8|<unk>|J|U|N|E|<unk>|1|9|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1\n",
      "***     *****EVAL Source[0]:     <s>|0|7|.|0|2|.|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|9|9|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7|-|0|7\n",
      "***     *****EVAL Source[0]:     <s>|M|A|Y|<unk>|1|8|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|0|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-|0|8|-\n",
      "***     *****EVAL Source[0]:     <s>|2|5|,|<unk>|o|c|t|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|1|-|0|0|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0|5|-|0\n",
      "***     *****EVAL Source[0]:     <s>|N|O|V|E|M|B|E|R|<unk>|1|<unk>|1|9|8|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|1|9|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|0|0|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|D|E|C|E|M|B|E|R|<unk>|1|9|9|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1|-|1|1\n",
      "***     *****EVAL Source[0]:     <s>|1|9|<unk>|S|E|P|T|E|M|B|E|R|<unk>|2|0|1|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|2|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2|9|-|2\n",
      "***     *****EVAL Source[0]:     <s>|a|p|r|i|l|<unk>|2|,|<unk>|1|9|8|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|1|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4|-|0|4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***     *****EVAL Source[0]:     <s>|T|u|e|s|d|a|y|,|<unk>|O|c|t|o|b|e|r|<unk>|2|6|,|<unk>|1|9|9|9|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2|1|-|2\n",
      "***     Validation loss: 12.6195 \n",
      "***     Eval BLUE score: 0.0522\n",
      "*** Epoch 8, global step 500: ***     Train loss: 11.0227 \n",
      "time per step = 0:00:0.100\n",
      "***     Train Source[0]:     <s>|0|4|,|<unk>|J|u|n|<unk>|2|0|1|7|<eos>\n",
      "***     Train Target[0]:     <s>|2|0|1|7|-|0|6|-|0|4|<eos>\n",
      "***     Train Prediction[0]: 0|0|1|7|-|0|6|-|0|6|-|6\n",
      "*** Running evaluation on a validation set:\n",
      "***     *****EVAL Source[0]:     <s>|3|1|.|1|0|.|0|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 0|0|0|5|-|1|0|-|1|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|M|A|Y|,|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|0|4|-|1|4|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|J|a|n|u|a|r|y|<unk>|2|3|,|<unk>|1|9|7|8|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|7|8|-|0|3|-|2|3|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|0|<unk>|j|u|l|y|<unk>|2|0|0|1|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|1|-|0|7|-|1|0|<eos>|<pad>|<pad>|<pad>\n",
      "***     *****EVAL Source[0]:     <s>|A|U|G|U|S|T|<unk>|2|7|<unk>|1|9|9|0|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|5|<unk>|2|0|1|7|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|7|-|0|5|-|1|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|8|<unk>|J|U|N|E|<unk>|1|9|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|1|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|0|7|.|0|2|.|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|3|-|0|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|M|A|Y|<unk>|1|8|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|2|5|,|<unk>|o|c|t|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|1|2|-|2|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|N|O|V|E|M|B|E|R|<unk>|1|<unk>|1|9|8|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|5|-|1|1|-|1|1|<eos>|<pad>|<pad>|<pad>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|1|9|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|1|1|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|D|E|C|E|M|B|E|R|<unk>|1|9|9|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|2|-|1|2|-|1|4|<eos>|<pad>|<pad>|<pad>\n",
      "***     *****EVAL Source[0]:     <s>|1|9|<unk>|S|E|P|T|E|M|B|E|R|<unk>|2|0|1|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|a|p|r|i|l|<unk>|2|,|<unk>|1|9|8|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|6|-|0|4|-|2|2|<eos>|<pad>|<pad>|<pad>|<pad>\n",
      "***     *****EVAL Source[0]:     <s>|T|u|e|s|d|a|y|,|<unk>|O|c|t|o|b|e|r|<unk>|2|6|,|<unk>|1|9|9|9|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|1|1|-|2|6|<eos>\n",
      "***     Validation loss: 2.6945 \n",
      "***     Eval BLUE score: 0.8153\n",
      "*** Epoch 16, global step 1000: ***     Train loss: 2.8750 \n",
      "time per step = 0:00:0.095\n",
      "***     Train Source[0]:     <s>|2|4|<unk>|0|5|<unk>|8|2|<eos>\n",
      "***     Train Target[0]:     <s>|1|9|8|2|-|0|5|-|2|4|<eos>\n",
      "***     Train Prediction[0]: 1|8|8|2|-|0|4|-|2|5|<eos>|<eos>\n",
      "*** Running evaluation on a validation set:\n",
      "***     *****EVAL Source[0]:     <s>|3|1|.|1|0|.|0|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|M|A|Y|,|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|J|a|n|u|a|r|y|<unk>|2|3|,|<unk>|1|9|7|8|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|0|<unk>|j|u|l|y|<unk>|2|0|0|1|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|A|U|G|U|S|T|<unk>|2|7|<unk>|1|9|9|0|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|5|<unk>|2|0|1|7|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|8|<unk>|J|U|N|E|<unk>|1|9|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|0|7|.|0|2|.|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|M|A|Y|<unk>|1|8|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|2|5|,|<unk>|o|c|t|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|N|O|V|E|M|B|E|R|<unk>|1|<unk>|1|9|8|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|1|9|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|D|E|C|E|M|B|E|R|<unk>|1|9|9|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|9|<unk>|S|E|P|T|E|M|B|E|R|<unk>|2|0|1|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|a|p|r|i|l|<unk>|2|,|<unk>|1|9|8|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|T|u|e|s|d|a|y|,|<unk>|O|c|t|o|b|e|r|<unk>|2|6|,|<unk>|1|9|9|9|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     Validation loss: 0.1573 \n",
      "***     Eval BLUE score: 0.9905\n",
      "*** Epoch 24, global step 1500: ***     Train loss: 0.1906 \n",
      "time per step = 0:00:0.095\n",
      "***     Train Source[0]:     <s>|J|A|N|U|A|R|Y|<unk>|2|7|,|<unk>|2|0|1|8|<eos>\n",
      "***     Train Target[0]:     <s>|2|0|1|8|-|0|1|-|2|7|<eos>\n",
      "***     Train Prediction[0]: 2|0|1|8|-|0|1|-|2|7|<eos>|-\n",
      "*** Running evaluation on a validation set:\n",
      "***     *****EVAL Source[0]:     <s>|3|1|.|1|0|.|0|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|M|A|Y|,|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|0|5|-|1|4|<eos>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***     *****EVAL Source[0]:     <s>|J|a|n|u|a|r|y|<unk>|2|3|,|<unk>|1|9|7|8|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|0|<unk>|j|u|l|y|<unk>|2|0|0|1|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|A|U|G|U|S|T|<unk>|2|7|<unk>|1|9|9|0|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|5|<unk>|2|0|1|7|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|8|<unk>|J|U|N|E|<unk>|1|9|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|0|7|.|0|2|.|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|M|A|Y|<unk>|1|8|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|2|5|,|<unk>|o|c|t|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|N|O|V|E|M|B|E|R|<unk>|1|<unk>|1|9|8|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|1|9|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|D|E|C|E|M|B|E|R|<unk>|1|9|9|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|9|<unk>|S|E|P|T|E|M|B|E|R|<unk>|2|0|1|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|a|p|r|i|l|<unk>|2|,|<unk>|1|9|8|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|T|u|e|s|d|a|y|,|<unk>|O|c|t|o|b|e|r|<unk>|2|6|,|<unk>|1|9|9|9|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     Validation loss: 0.0153 \n",
      "***     Eval BLUE score: 0.9999\n",
      "*** Epoch 32, global step 2000: ***     Train loss: 0.1004 \n",
      "time per step = 0:00:0.095\n",
      "***     Train Source[0]:     <s>|1|5|.|0|2|.|9|6|<eos>\n",
      "***     Train Target[0]:     <s>|1|9|9|6|-|0|2|-|1|5|<eos>\n",
      "***     Train Prediction[0]: 1|9|9|6|-|0|2|-|1|5|<eos>|-\n",
      "*** Running evaluation on a validation set:\n",
      "***     *****EVAL Source[0]:     <s>|3|1|.|1|0|.|0|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|5|-|1|0|-|3|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|M|A|Y|,|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|0|5|-|1|4|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|J|a|n|u|a|r|y|<unk>|2|3|,|<unk>|1|9|7|8|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|7|8|-|0|1|-|2|3|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|0|<unk>|j|u|l|y|<unk>|2|0|0|1|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|1|-|0|7|-|1|0|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|A|U|G|U|S|T|<unk>|2|7|<unk>|1|9|9|0|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|0|-|0|8|-|2|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|5|<unk>|2|0|1|7|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|7|-|1|0|-|0|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|8|<unk>|J|U|N|E|<unk>|1|9|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|6|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|0|7|.|0|2|.|9|3|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|3|-|0|2|-|0|7|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|M|A|Y|<unk>|1|8|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|0|5|-|1|8|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|2|5|,|<unk>|o|c|t|<unk>|1|9|8|4|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|4|-|1|0|-|2|5|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|N|O|V|E|M|B|E|R|<unk>|1|<unk>|1|9|8|5|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|5|-|1|1|-|0|1|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|o|c|t|o|b|e|r|<unk>|1|9|,|<unk>|2|0|0|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|0|6|-|1|0|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|4|<unk>|D|E|C|E|M|B|E|R|<unk>|1|9|9|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|2|-|1|2|-|1|4|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|1|9|<unk>|S|E|P|T|E|M|B|E|R|<unk>|2|0|1|2|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Prediction[0]: 2|0|1|2|-|0|9|-|1|9|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|a|p|r|i|l|<unk>|2|,|<unk>|1|9|8|6|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|8|6|-|0|4|-|0|2|<eos>\n",
      "***     *****EVAL Source[0]:     <s>|T|u|e|s|d|a|y|,|<unk>|O|c|t|o|b|e|r|<unk>|2|6|,|<unk>|1|9|9|9|<eos>\n",
      "***     *****EVAL Target[0]:     <s>|1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     *****EVAL Prediction[0]: 1|9|9|9|-|1|0|-|2|6|<eos>\n",
      "***     Validation loss: 0.0035 \n",
      "***     Eval BLUE score: 0.9999\n",
      "INFO:tensorflow:Saving checkpoints for 2480 into dates_eval_log/model.ckpt.\n",
      "*** Finished training\n",
      "*** Avg time per step: 0.096s\n",
      "*** Avg objects per second: 35698.713\n",
      "CPU times: user 8min 27s, sys: 39.7 s, total: 9min 6s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "run_open_seq2seq(\n",
    "    new_run, \n",
    "    '--config_file=dates_eval_config.py --mode=train_eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, look for lines marked with 'Eval BLUE score'.  Observe how that score increases during the training run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Use Case\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We now have a better understanding of what attention can be used for.  Next we will try an actual Neural Machine Translation model for translating from German to English.\n",
    "\n",
    "But first, we need to discuss an important point in Machine Translation as well as other NLP tasks:  BPE representation.\n",
    "\n",
    "### BPE\n",
    "BPE stands for byte pair encoding. It means that common byte pairs - bigrams of characters in our case - are replaced by a byte which never occurs in the corpus. For example, in our corpus we have never seen the \"#\" char so we could use it to represent some typical bigram like \"ie\". In practice, however, all the printable characters are used, so we use the unprintable part of the codepage for BPE.  In order actually print the text, we need to reformat it back. So you'll see \"@@\" in text - these are artifacts from such renormalization.\n",
    "\n",
    "Here is an example text in German, which will be translated in English by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Premierminister Indi@@ ens und Japans tra@@ fen sich in Tok@@ io .\r\n",
      "Indi@@ ens neuer Premierminister Nar@@ end@@ ra Mo@@ di trifft bei seinem ersten wichtigen Auslands@@ besu@@ ch seit seinem Wahl@@ sie@@ g im Mai seinen japanischen Amts@@ kol@@ legen Shin@@ zo A@@ be in Tok@@ o , um wirtschaftliche und sicherheits@@ politische Beziehungen zu bes@@ prechen .\r\n",
      "Herr Mo@@ di befindet sich auf einer fünf@@ tä@@ gigen Reise nach Japan , um die wirtschaftlichen Beziehungen mit der d@@ ritt@@ größten Wirtschafts@@ nation der Welt zu festigen .\r\n",
      "Pläne für eine stärkere kern@@ technische Zusammenarbeit stehen ganz oben auf der Tagesordnung .\r\n",
      "Berichten zufolge ho@@ fft Indien darüber hinaus auf einen Vertrag zur Verteidigungs@@ zusammenarbeit zwischen den beiden Nationen .\r\n",
      "Polizei von Kar@@ ra@@ tha verhaftet 20-@@ J@@ ähri@@ gen nach schneller Motor@@ rad@@ j@@ ag@@ d\r\n",
      "Ein Motor@@ rad wurde besch@@ la@@ gn@@ ah@@ mt , nachdem der Fahrer es mit 125 km / h in einer 70 km / h-@@ Zone und durch Bu@@ sch@@ land gefahren hatte , um der Polizei in Bil@@ bara zu ent@@ kommen .\r\n",
      "Verkehrs@@ poli@@ zi@@ sten in Kar@@ ra@@ tha versu@@ chten heute morgen , ein blau@@ es Motor@@ rad zu stoppen , nachdem sie es dabei beobachtet hatten , wie es mit 125 km / h eine Tank@@ stelle auf der Ba@@ th@@ date Road ver@@ ließ .\r\n",
      "Die Polizei berichtet , dass der Fahrer die Hal@@ te@@ sign@@ ale dann ignori@@ erte und weiter auf der Bur@@ ges@@ s Road fuhr , bevor er in das Bu@@ sch@@ land ab@@ bo@@ g , wo die Beamten es aus den Augen verloren .\r\n",
      "Das Motor@@ rad sowie eine Person , die der Beschreibung des Fahr@@ ers entsp@@ ra@@ ch wurden später bei einem Haus im Wal@@ cot@@ t Way in Bul@@ gar@@ ra gesehen .\r\n"
     ]
    }
   ],
   "source": [
    "!head /dli/data/wmt/newstest2015.tok.bpe.32000.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's see what our results are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">==================> Running in inference mode\n",
      ">==================> Executing training mode\n",
      ">==================> Creating data layer\n",
      "WARNING: skipped 0 pairs with max source size of 0 and max target size of 0\n",
      ">==================> Data layer created\n",
      ">==================> Building graph on GPU:0\n",
      "Inference Mode. Loss part of graph isn't built.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      ">==================> Trying to restore from: /dli/data/noatt/model-59244\n",
      "INFO:tensorflow:Restoring parameters from /dli/data/noatt/model-59244\n",
      ">==================> Saving inference results to: baseline.txt\n",
      "Prime Minister India and Japan met in Tokyo . </S> </S> </S>\n",
      "Federal Government and Bun@@ de@@ stag not joined . </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S> </S>\n",
      "I wonder him if he will ever make a booth up com@@ ed@@ y@@ tour . </S> </S> </S> </S> </S>\n",
      "A@@ il@@ inn Sol@@ om@@ ons was a delicate beauty with un@@ attended hair and a flat heart from a village on the G@@ ordin@@ sel , which was even remote and rough than Port Re@@ ub@@ en . </S> </S>\n",
      "Some gran@@ at@@ ers are still in my kne@@ e . </S>\n",
      "If your camera has a remote mode , using a series of images with slightly changing exposure settings , use it . </S> </S> </S> </S>\n",
      "And those who resist claims know that the time is for them , but certainly not with the pla@@ ins . </S> </S> </S>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-b69823574b71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m run_open_seq2seq(\n\u001b[1;32m      2\u001b[0m     \u001b[0mold_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     '--config_file=old_s2s/example_configs/nmt.json --logdir=/dli/data/nmt --mode=infer --inference_out=pred.txt')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-405c987fc99e>\u001b[0m in \u001b[0;36mrun_open_seq2seq\u001b[0;34m(version, args)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dli/tasks/task3/task/old_s2s/run.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mdeco_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running in inference mode\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown mode in config file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dli/tasks/task3/task/old_s2s/run.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    228\u001b[0m                            feed_dict={\n\u001b[1;32m    229\u001b[0m                                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                                \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                            })\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_out\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"stdout\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    581\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1060\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_open_seq2seq(\n",
    "    old_run, \n",
    "    '--config_file=old_s2s/example_configs/nmt.json --logdir=/dli/data/nmt --mode=infer --inference_out=pred.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open up [pred.txt](pred.txt) to compare to [baseline.txt](baseline.txt) to see the difference attention makes in the overall quality of the translation. Training with more epochs will improve and possibly to get to the translation that you've identified as the best in the first part of the lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment - BLEU score\n",
    "\n",
    "In this final part, you are going to implement the Bilingual Evaluation Understudy, or BLUE, Score to assess the effectiveness of translations as a ratio. Completely mismatched results will be scored as 0.0, and the perfect translation will be scored as 1.0. There are various formulas applied to sentence, corpus or any length translation, but it's meant to be a good proxy to compare various implementations. Our date translation examples are simple, but BLEU score is also meant to encapsulate how a human would rate the translation for language translations.\n",
    "\n",
    "<img src=\"images/BLEU.png\" style=\"width:600;height:300px;\">\n",
    "\n",
    "The BLEU could be described as the overlap of single tokens and sequences of tokens (2, 3 or 4) - this is $precision_i$.\n",
    "And also brevity penalty of $\\frac{output-length}{reference - length}$ is applied in the formula.\n",
    "\n",
    "We will split each ground truth to one character at a time and also the predicted value. BLEU score works on the 1-4 ngrams so we need to supply at least 4 ngrams in our predictions. If we had a string 2007-07-11, we can split it into tokens 2007, 07 and 11. For exact matches, the score is 1.0 but for small variations like one character off, it makes the score lower than it should be. We will call this string score implemented in `str_score`.\n",
    "\n",
    "Alternatively, we can convert it to list of ['2', '0', '0', '7', '-', '0', '7', '-', '1', '1'] by making a token of every single character, including dashes to get the higher score. It should be implemented in `char_score`. Let's compare the two implementations and the BLEU scores they calculate.\n",
    "\n",
    "For your reference: parameters for sentence_bleu function are as follows:\t\n",
    "references (list(list(str))) – A list of reference translations<br>\n",
    "hypothesis (list(str)) – A hypothesis translation\n",
    "\n",
    "### Generate the right BLEU scores\n",
    "\n",
    "Change the code for how the strings are being prepared and compared in [my_bleu.py](../../../../edit/tasks/task3/task/my_bleu.py) for both type of BLEU score calculations. Save your changes to fix BLEU score calculation.\n",
    "\n",
    "Hint: Look at the actual output compared to the expected reference/output on which BLEU score is being checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006737946999085467\n",
      "0.006737946999085467\n",
      "0.005930146235786944\n",
      "0.005930146235786944\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation\n",
    "# Kernel->restart will ensure the latest version of my_bleu.py is imported if you made any changes to the code to re-test\n",
    "\n",
    "import my_bleu as mb\n",
    "\n",
    "output1 = '2007-07-11<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
    "expected = '2007-07-11'\n",
    "\n",
    "print(mb.str_score(output1, expected))    #expecting 1.0\n",
    "print(mb.char_score(output1, expected)) # expecting 1.0\n",
    "\n",
    "output2 = '2007-07-12<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>' #it's 12 instead of 011\n",
    "\n",
    "print(mb.str_score(output2, expected))    #expecting 0.7598356856515925\n",
    "print(mb.char_score(output2, expected)) #expecting 0.8801117367933934"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Optional] Experiment with Beam Search\n",
    "\n",
    "As we've seen, attention changes the probability of which character to focus on in a given sequence. There are other strategies we can apply to improve the results of the translation. Beam search is one such popular heuristic. It works by returning a list of output *sequences* that are most likely for a given input. It does this by exploring all of the possible next steps, and then chooses the most promising $k$ candidates from all possibilities. Those candidates form a 'beam' of directions, where the algorithm expands the search through each candidate's sequence of probabilities. \n",
    "\n",
    "The larger the beam, the better the performance of the model. Since there are more candidate sequences this means the likelihood of finding the right answer increases. However, it requires more decoding, meaning a direct tradeoff with the quality of results versus accuracy. Seeding beam search with the most likely next words in a sequence can yield good results. The beam search would generate the resulting sequence, starting from the first word and appending, while exploring all the candidates (size of the beam) every step of the way.\n",
    "\n",
    "We have started the beginning of beam search to generate samples with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "Embedding_Layer (Embedding)  (None, 20, 64)            3968      \n",
      "_________________________________________________________________\n",
      "Encoder (Bidirectional)      (None, 20, 64)            24832     \n",
      "_________________________________________________________________\n",
      "Decoder (LSTM)               (None, 20, 32)            12416     \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 20, 15)            495       \n",
      "=================================================================\n",
      "Total params: 41,711\n",
      "Trainable params: 41,711\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "m = model_simple_nmt(len(human_vocab), len(machine_vocab))\n",
    "\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = zip(*dataset)\n",
    "inputs = np.array([string_to_int(i, 20, human_vocab) for i in inputs])\n",
    "targets = [string_to_int(t, 20, machine_vocab) for t in targets]\n",
    "targets = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 17s 2ms/step - loss: 1.4407 - acc: 0.5634 - val_loss: 1.0408 - val_acc: 0.6407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd4d69aa748>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit([inputs], targets, epochs=1, batch_size=64, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_rnn_predict(samples, empty=human_vocab[\"<pad>\"], rnn_model=m, maxlen=30):\n",
    "    \"\"\"for every sample, calculate probability for every possible label\n",
    "    you need to supply your RNN model and maxlen - the length of sequences it can handle\n",
    "    \"\"\"\n",
    "    data = sequence.pad_sequences(samples, maxlen=maxlen, value=empty)\n",
    "    return rnn_model.predict(data, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll try to generate some dates from our model to demonstrate beam search properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamsearch(predict=keras_rnn_predict, k=1, maxsample=10, \n",
    "               use_unk=False, \n",
    "               oov=human_vocab[\"<unk>\"], \n",
    "               empty=human_vocab[\"<pad>\"], \n",
    "               eos=human_vocab[\"<unk>\"]):\n",
    "    \"\"\"return k samples (beams) and their NLL scores, each sample is a sequence of labels,\n",
    "    all samples starts with an `empty` label and end with `eos` or truncated to length of `maxsample`.\n",
    "    You need to supply `predict` which returns the label probability of each sample.\n",
    "    `use_unk` allow usage of `oov` (out-of-vocabulary) label in samples\n",
    "    \"\"\"\n",
    "    \n",
    "    dead_k = 0 # samples that reached eos\n",
    "    dead_samples = []\n",
    "    dead_scores = []\n",
    "    live_k = 1 # samples that did not yet reached eos\n",
    "    live_samples = [[empty]]\n",
    "    live_scores = [0]\n",
    "\n",
    "    while live_k and dead_k < k:\n",
    "        # for every possible live sample calc prob for every possible label \n",
    "        probs = predict(live_samples, empty=empty)\n",
    "\n",
    "        # total score for every sample is sum of -log of word prb\n",
    "        cand_scores = np.array(live_scores)[:,None] - np.log(probs)\n",
    "        if not use_unk and oov is not None:\n",
    "            cand_scores[:,oov] = 1e20\n",
    "        cand_flat = cand_scores.flatten()\n",
    "\n",
    "        # find the best (lowest) scores we have from all possible samples and new words\n",
    "        ranks_flat = cand_flat.argsort()[:(k-dead_k)]\n",
    "        live_scores = cand_flat[ranks_flat]\n",
    "\n",
    "        # append the new words to their appropriate live sample\n",
    "        voc_size = probs.shape[1]\n",
    "        live_samples = [live_samples[r//voc_size]+[r%voc_size] for r in ranks_flat]\n",
    "\n",
    "        # live samples that should be dead are...\n",
    "        zombie = [s[-1] == eos or len(s) >= maxsample for s in live_samples]\n",
    "        \n",
    "        # add zombies to the dead\n",
    "        dead_samples += [s for s,z in zip(live_samples,zombie) if z]  # remove first label == empty\n",
    "        dead_scores += [s for s,z in zip(live_scores,zombie) if z]\n",
    "        dead_k = len(dead_samples)\n",
    "        # remove zombies from the living \n",
    "        live_samples = [s for s,z in zip(live_samples,zombie) if not z]\n",
    "        live_scores = [s for s,z in zip(live_scores,zombie) if not z]\n",
    "        live_k = len(live_samples)\n",
    "\n",
    "    return live_samples + dead_samples, live_scores + dead_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this lab, we discussed the use of Deep Learning for performing Machine Learning.  We experimented with a simple problem of converting dates from a human to a machine-readable form.  We then improved the model using attention. Finally, we explored the use of beam search.\n",
    "\n",
    "In this lab we focus on *supervised* Machine Translation.  This directly implies that we need labeled corpora.  However, for languages with little or no labeled data, this is very unlikely.  A fascinating avenue of research is the use of *unsupervised* MT, which does not require such corpora.    \n",
    "\n",
    "Machine Translation is an enabling tool to bring various people and cultures around the world closer together.  The tools and techniques you have seen in this lab can help make this a reality.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Acknowledgements__: \n",
    "\n",
    "The idea of date translation is borrowed from https://github.com/datalogue/keras-attention.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
