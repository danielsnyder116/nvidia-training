{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The cat sat on the mat', 'The dog sat on the mat', 'The goat sat on the mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This countvectorizer takes care of a lot of preprocessing under the hood\n",
    "# stop_words, tokenizing, lowercasing everything\n",
    "vectorizer = CountVectorizer(lowercase=True, analyzer='word', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['goat', 'sat', 'cat', 'mat', 'on', 'dog', 'the'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>goat</th>\n",
       "      <th>mat</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat  dog  goat  mat  on  sat  the\n",
       "0    1    0     0    1   1    1    2\n",
       "1    0    1     0    1   1    1    2\n",
       "2    0    0     1    1   1    1    2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fitting and transforming model\n",
    "representation = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.vocabulary_.keys())\n",
    "\n",
    "df_rep = pd.DataFrame(data=representation.toarray(), columns=sorted(vectorizer.vocabulary_.keys()))\n",
    "\n",
    "#Shows the frequency that each word shows up in the sentence\n",
    "df_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This also gets rid of stopwords\n",
    "vectorizer = CountVectorizer(lowercase=True, analyzer='word', binary=True,\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = vectorizer.fit_transform(corpus)\n",
    "\n",
    "df_rep = pd.DataFrame(data=rep.toarray(), columns=vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sat</th>\n",
       "      <th>mat</th>\n",
       "      <th>goat</th>\n",
       "      <th>dog</th>\n",
       "      <th>cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sat  mat  goat  dog  cat\n",
       "0    1    0     0    1    1\n",
       "1    0    1     0    1    1\n",
       "2    0    0     1    1    1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a binary version, so no longer number of times the word shows up in sentence,\n",
    "# but binary variable of whether or not sentence appears in each sentence\n",
    "df_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': 0, 'dog': 1, 'goat': 2, 'mat': 3, 'sat': 4}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>bird</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>elephant</th>\n",
       "      <th>goat</th>\n",
       "      <th>keyboard</th>\n",
       "      <th>mat</th>\n",
       "      <th>notebook</th>\n",
       "      <th>pen</th>\n",
       "      <th>plane</th>\n",
       "      <th>sat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   apple  bird  cat  dog  elephant  goat  keyboard  mat  notebook  pen  plane  \\\n",
       "0      0     0    1    0         0     0         0    1         0    0      0   \n",
       "1      0     0    0    1         0     0         0    1         0    0      0   \n",
       "2      0     0    0    0         0     1         0    1         0    0      0   \n",
       "3      0     0    0    0         1     0         0    1         0    0      0   \n",
       "4      0     0    0    0         0     0         0    1         0    0      1   \n",
       "5      1     0    0    0         0     0         0    1         0    0      0   \n",
       "6      0     0    0    0         0     0         0    1         0    1      0   \n",
       "7      0     0    0    0         0     0         0    1         1    0      0   \n",
       "8      0     0    0    0         0     0         1    0         0    0      0   \n",
       "9      0     1    0    0         0     0         0    0         0    0      0   \n",
       "\n",
       "   sat  \n",
       "0    1  \n",
       "1    1  \n",
       "2    1  \n",
       "3    1  \n",
       "4    1  \n",
       "5    1  \n",
       "6    1  \n",
       "7    1  \n",
       "8    0  \n",
       "9    0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_corpus = ['The cat sat on the mat', 'The dog sat on the mat', 'The goat sat on the mat', 'The elephant sat on the mat', \n",
    "          'The plane sat on the mat', 'The apple sat on the mat', 'The pen sat on the mat', 'The notebook sat on the mat']\n",
    "\n",
    "allowed = [1,1,1,1,   # Objects that are allowed on the mat\n",
    "           0,0,0,0]   # Objects that are not allowed on the mat\n",
    "\n",
    "# Make sure that words we'll use in the test set are considered\n",
    "for other_object in ['keyboard', 'bird']:\n",
    "    training_corpus.append(other_object)   \n",
    "    \n",
    "vectorizer = CountVectorizer(lowercase=True, analyzer='word', binary=True, stop_words='english')\n",
    "representation = vectorizer.fit_transform(training_corpus)\n",
    "representation_df = pd.DataFrame(data = representation.toarray(), columns=sorted(vectorizer.vocabulary_.keys()))\n",
    "representation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Score:100.0 %\n"
     ]
    }
   ],
   "source": [
    "#Text Classification Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logr = LogisticRegression()\n",
    "\n",
    "y = allowed\n",
    "X = representation_df[:len(y)]\n",
    "\n",
    "logr.fit(X,y)\n",
    "\n",
    "print(\"Training Accuracy Score:{} %\".format(accuracy_score(logr.predict(X), y)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Results for (keyboard, bird):  [0, 1]\n",
      "Actual   Results for (keyboard, bird):  [0 0]\n"
     ]
    }
   ],
   "source": [
    "#Now that we've fit our model, we want to test it on new words.\n",
    "#Since we've already fit our vectorizer, we now just use .transform \n",
    "#to convert the new strings to a one-hot encoded matrix\n",
    "\n",
    "test_corpus = ['The keyboard sat on the mat', 'The bird sat on the mat']\n",
    "\n",
    "rep = vectorizer.transform(test_corpus)\n",
    "\n",
    "X_test =  rep\n",
    "y_test = [0,1]\n",
    "print(\"Expected Results for (keyboard, bird):  {}\".format(y_test))\n",
    "print(\"Actual   Results for (keyboard, bird):  {}\".format(logr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 128 sentences in the corpus, with a vocabulary of 18 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cat', 'sat', 'mat'),\n",
       " ('cat', 'sat', 'rug'),\n",
       " ('cat', 'sat', 'sofa'),\n",
       " ('cat', 'sat', 'bed'),\n",
       " ('cat', 'stood', 'mat'),\n",
       " ('cat', 'stood', 'rug'),\n",
       " ('cat', 'stood', 'sofa'),\n",
       " ('cat', 'stood', 'bed'),\n",
       " ('cat', 'jumped', 'mat'),\n",
       " ('cat', 'jumped', 'rug'),\n",
       " ('cat', 'jumped', 'sofa'),\n",
       " ('cat', 'jumped', 'bed'),\n",
       " ('cat', 'slept', 'mat'),\n",
       " ('cat', 'slept', 'rug'),\n",
       " ('cat', 'slept', 'sofa'),\n",
       " ('cat', 'slept', 'bed'),\n",
       " ('dog', 'sat', 'mat'),\n",
       " ('dog', 'sat', 'rug'),\n",
       " ('dog', 'sat', 'sofa'),\n",
       " ('dog', 'sat', 'bed'),\n",
       " ('dog', 'stood', 'mat'),\n",
       " ('dog', 'stood', 'rug'),\n",
       " ('dog', 'stood', 'sofa'),\n",
       " ('dog', 'stood', 'bed'),\n",
       " ('dog', 'jumped', 'mat'),\n",
       " ('dog', 'jumped', 'rug'),\n",
       " ('dog', 'jumped', 'sofa'),\n",
       " ('dog', 'jumped', 'bed'),\n",
       " ('dog', 'slept', 'mat'),\n",
       " ('dog', 'slept', 'rug'),\n",
       " ('dog', 'slept', 'sofa'),\n",
       " ('dog', 'slept', 'bed'),\n",
       " ('goat', 'sat', 'mat'),\n",
       " ('goat', 'sat', 'rug'),\n",
       " ('goat', 'sat', 'sofa'),\n",
       " ('goat', 'sat', 'bed'),\n",
       " ('goat', 'stood', 'mat'),\n",
       " ('goat', 'stood', 'rug'),\n",
       " ('goat', 'stood', 'sofa'),\n",
       " ('goat', 'stood', 'bed'),\n",
       " ('goat', 'jumped', 'mat'),\n",
       " ('goat', 'jumped', 'rug'),\n",
       " ('goat', 'jumped', 'sofa'),\n",
       " ('goat', 'jumped', 'bed'),\n",
       " ('goat', 'slept', 'mat'),\n",
       " ('goat', 'slept', 'rug'),\n",
       " ('goat', 'slept', 'sofa'),\n",
       " ('goat', 'slept', 'bed'),\n",
       " ('elephant', 'sat', 'mat'),\n",
       " ('elephant', 'sat', 'rug'),\n",
       " ('elephant', 'sat', 'sofa'),\n",
       " ('elephant', 'sat', 'bed'),\n",
       " ('elephant', 'stood', 'mat'),\n",
       " ('elephant', 'stood', 'rug'),\n",
       " ('elephant', 'stood', 'sofa'),\n",
       " ('elephant', 'stood', 'bed'),\n",
       " ('elephant', 'jumped', 'mat'),\n",
       " ('elephant', 'jumped', 'rug'),\n",
       " ('elephant', 'jumped', 'sofa'),\n",
       " ('elephant', 'jumped', 'bed'),\n",
       " ('elephant', 'slept', 'mat'),\n",
       " ('elephant', 'slept', 'rug'),\n",
       " ('elephant', 'slept', 'sofa'),\n",
       " ('elephant', 'slept', 'bed'),\n",
       " ('eagle', 'sat', 'mat'),\n",
       " ('eagle', 'sat', 'rug'),\n",
       " ('eagle', 'sat', 'sofa'),\n",
       " ('eagle', 'sat', 'bed'),\n",
       " ('eagle', 'stood', 'mat'),\n",
       " ('eagle', 'stood', 'rug'),\n",
       " ('eagle', 'stood', 'sofa'),\n",
       " ('eagle', 'stood', 'bed'),\n",
       " ('eagle', 'jumped', 'mat'),\n",
       " ('eagle', 'jumped', 'rug'),\n",
       " ('eagle', 'jumped', 'sofa'),\n",
       " ('eagle', 'jumped', 'bed'),\n",
       " ('eagle', 'slept', 'mat'),\n",
       " ('eagle', 'slept', 'rug'),\n",
       " ('eagle', 'slept', 'sofa'),\n",
       " ('eagle', 'slept', 'bed'),\n",
       " ('zebra', 'sat', 'mat'),\n",
       " ('zebra', 'sat', 'rug'),\n",
       " ('zebra', 'sat', 'sofa'),\n",
       " ('zebra', 'sat', 'bed'),\n",
       " ('zebra', 'stood', 'mat'),\n",
       " ('zebra', 'stood', 'rug'),\n",
       " ('zebra', 'stood', 'sofa'),\n",
       " ('zebra', 'stood', 'bed'),\n",
       " ('zebra', 'jumped', 'mat'),\n",
       " ('zebra', 'jumped', 'rug'),\n",
       " ('zebra', 'jumped', 'sofa'),\n",
       " ('zebra', 'jumped', 'bed'),\n",
       " ('zebra', 'slept', 'mat'),\n",
       " ('zebra', 'slept', 'rug'),\n",
       " ('zebra', 'slept', 'sofa'),\n",
       " ('zebra', 'slept', 'bed'),\n",
       " ('rhino', 'sat', 'mat'),\n",
       " ('rhino', 'sat', 'rug'),\n",
       " ('rhino', 'sat', 'sofa'),\n",
       " ('rhino', 'sat', 'bed'),\n",
       " ('rhino', 'stood', 'mat'),\n",
       " ('rhino', 'stood', 'rug'),\n",
       " ('rhino', 'stood', 'sofa'),\n",
       " ('rhino', 'stood', 'bed'),\n",
       " ('rhino', 'jumped', 'mat'),\n",
       " ('rhino', 'jumped', 'rug'),\n",
       " ('rhino', 'jumped', 'sofa'),\n",
       " ('rhino', 'jumped', 'bed'),\n",
       " ('rhino', 'slept', 'mat'),\n",
       " ('rhino', 'slept', 'rug'),\n",
       " ('rhino', 'slept', 'sofa'),\n",
       " ('rhino', 'slept', 'bed'),\n",
       " ('hippo', 'sat', 'mat'),\n",
       " ('hippo', 'sat', 'rug'),\n",
       " ('hippo', 'sat', 'sofa'),\n",
       " ('hippo', 'sat', 'bed'),\n",
       " ('hippo', 'stood', 'mat'),\n",
       " ('hippo', 'stood', 'rug'),\n",
       " ('hippo', 'stood', 'sofa'),\n",
       " ('hippo', 'stood', 'bed'),\n",
       " ('hippo', 'jumped', 'mat'),\n",
       " ('hippo', 'jumped', 'rug'),\n",
       " ('hippo', 'jumped', 'sofa'),\n",
       " ('hippo', 'jumped', 'bed'),\n",
       " ('hippo', 'slept', 'mat'),\n",
       " ('hippo', 'slept', 'rug'),\n",
       " ('hippo', 'slept', 'sofa'),\n",
       " ('hippo', 'slept', 'bed')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = ['cat','dog','goat','elephant','eagle','zebra','rhino', 'hippo']\n",
    "actions = ['sat','stood','jumped','slept']\n",
    "furniture = ['mat','rug','sofa','bed']\n",
    "\n",
    "# Generate all combinations of animal, action and furniture\n",
    "animal_corpus = ['The {} {} on the {}'.format(x[0], x[1], x[2]) for x in itertools.product(animals, actions, furniture)]\n",
    "vocabulary_size = len(animals) + len(actions) + len(furniture) + 2\n",
    "\n",
    "print(\"There are {} sentences in the corpus, with a vocabulary of {} words\".format(len(animal_corpus), vocabulary_size))\n",
    "\n",
    "#So you can use product to get every single combination of three words\n",
    "list(product(animals, actions, furniture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "import numpy as np\n",
    "\n",
    "# Hyper-parameters\n",
    "\n",
    "EMBEDDING_SIZE = 7  # Small corpus, so we're using a small dimension\n",
    "WINDOW_SIZE = 4     # Empirically found to work well\n",
    "\n",
    "# Convert text to numerical sequences\n",
    "\n",
    "# Note that the Tokenizer starts numbering words with 1.  So we have vocabulary_size+1 words.  The 0-th word\n",
    "# is considered to be the 'Out-of-vocabulary' token.\n",
    "tokenizer = Tokenizer(num_words=vocabulary_size+1, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', lower=True, split=' ',)\n",
    "tokenizer.fit_on_texts(animal_corpus)\n",
    "sequences = tokenizer.texts_to_sequences(animal_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 11, 3, 2, 1, 4],\n",
       " [1, 11, 3, 2, 1, 5],\n",
       " [1, 11, 3, 2, 1, 6],\n",
       " [1, 11, 3, 2, 1, 7],\n",
       " [1, 11, 8, 2, 1, 4],\n",
       " [1, 11, 8, 2, 1, 5],\n",
       " [1, 11, 8, 2, 1, 6],\n",
       " [1, 11, 8, 2, 1, 7],\n",
       " [1, 11, 9, 2, 1, 4],\n",
       " [1, 11, 9, 2, 1, 5],\n",
       " [1, 11, 9, 2, 1, 6],\n",
       " [1, 11, 9, 2, 1, 7],\n",
       " [1, 11, 10, 2, 1, 4],\n",
       " [1, 11, 10, 2, 1, 5],\n",
       " [1, 11, 10, 2, 1, 6],\n",
       " [1, 11, 10, 2, 1, 7],\n",
       " [1, 12, 3, 2, 1, 4],\n",
       " [1, 12, 3, 2, 1, 5],\n",
       " [1, 12, 3, 2, 1, 6],\n",
       " [1, 12, 3, 2, 1, 7],\n",
       " [1, 12, 8, 2, 1, 4],\n",
       " [1, 12, 8, 2, 1, 5],\n",
       " [1, 12, 8, 2, 1, 6],\n",
       " [1, 12, 8, 2, 1, 7],\n",
       " [1, 12, 9, 2, 1, 4],\n",
       " [1, 12, 9, 2, 1, 5],\n",
       " [1, 12, 9, 2, 1, 6],\n",
       " [1, 12, 9, 2, 1, 7],\n",
       " [1, 12, 10, 2, 1, 4],\n",
       " [1, 12, 10, 2, 1, 5],\n",
       " [1, 12, 10, 2, 1, 6],\n",
       " [1, 12, 10, 2, 1, 7],\n",
       " [1, 13, 3, 2, 1, 4],\n",
       " [1, 13, 3, 2, 1, 5],\n",
       " [1, 13, 3, 2, 1, 6],\n",
       " [1, 13, 3, 2, 1, 7],\n",
       " [1, 13, 8, 2, 1, 4],\n",
       " [1, 13, 8, 2, 1, 5],\n",
       " [1, 13, 8, 2, 1, 6],\n",
       " [1, 13, 8, 2, 1, 7],\n",
       " [1, 13, 9, 2, 1, 4],\n",
       " [1, 13, 9, 2, 1, 5],\n",
       " [1, 13, 9, 2, 1, 6],\n",
       " [1, 13, 9, 2, 1, 7],\n",
       " [1, 13, 10, 2, 1, 4],\n",
       " [1, 13, 10, 2, 1, 5],\n",
       " [1, 13, 10, 2, 1, 6],\n",
       " [1, 13, 10, 2, 1, 7],\n",
       " [1, 14, 3, 2, 1, 4],\n",
       " [1, 14, 3, 2, 1, 5],\n",
       " [1, 14, 3, 2, 1, 6],\n",
       " [1, 14, 3, 2, 1, 7],\n",
       " [1, 14, 8, 2, 1, 4],\n",
       " [1, 14, 8, 2, 1, 5],\n",
       " [1, 14, 8, 2, 1, 6],\n",
       " [1, 14, 8, 2, 1, 7],\n",
       " [1, 14, 9, 2, 1, 4],\n",
       " [1, 14, 9, 2, 1, 5],\n",
       " [1, 14, 9, 2, 1, 6],\n",
       " [1, 14, 9, 2, 1, 7],\n",
       " [1, 14, 10, 2, 1, 4],\n",
       " [1, 14, 10, 2, 1, 5],\n",
       " [1, 14, 10, 2, 1, 6],\n",
       " [1, 14, 10, 2, 1, 7],\n",
       " [1, 15, 3, 2, 1, 4],\n",
       " [1, 15, 3, 2, 1, 5],\n",
       " [1, 15, 3, 2, 1, 6],\n",
       " [1, 15, 3, 2, 1, 7],\n",
       " [1, 15, 8, 2, 1, 4],\n",
       " [1, 15, 8, 2, 1, 5],\n",
       " [1, 15, 8, 2, 1, 6],\n",
       " [1, 15, 8, 2, 1, 7],\n",
       " [1, 15, 9, 2, 1, 4],\n",
       " [1, 15, 9, 2, 1, 5],\n",
       " [1, 15, 9, 2, 1, 6],\n",
       " [1, 15, 9, 2, 1, 7],\n",
       " [1, 15, 10, 2, 1, 4],\n",
       " [1, 15, 10, 2, 1, 5],\n",
       " [1, 15, 10, 2, 1, 6],\n",
       " [1, 15, 10, 2, 1, 7],\n",
       " [1, 16, 3, 2, 1, 4],\n",
       " [1, 16, 3, 2, 1, 5],\n",
       " [1, 16, 3, 2, 1, 6],\n",
       " [1, 16, 3, 2, 1, 7],\n",
       " [1, 16, 8, 2, 1, 4],\n",
       " [1, 16, 8, 2, 1, 5],\n",
       " [1, 16, 8, 2, 1, 6],\n",
       " [1, 16, 8, 2, 1, 7],\n",
       " [1, 16, 9, 2, 1, 4],\n",
       " [1, 16, 9, 2, 1, 5],\n",
       " [1, 16, 9, 2, 1, 6],\n",
       " [1, 16, 9, 2, 1, 7],\n",
       " [1, 16, 10, 2, 1, 4],\n",
       " [1, 16, 10, 2, 1, 5],\n",
       " [1, 16, 10, 2, 1, 6],\n",
       " [1, 16, 10, 2, 1, 7],\n",
       " [1, 17, 3, 2, 1, 4],\n",
       " [1, 17, 3, 2, 1, 5],\n",
       " [1, 17, 3, 2, 1, 6],\n",
       " [1, 17, 3, 2, 1, 7],\n",
       " [1, 17, 8, 2, 1, 4],\n",
       " [1, 17, 8, 2, 1, 5],\n",
       " [1, 17, 8, 2, 1, 6],\n",
       " [1, 17, 8, 2, 1, 7],\n",
       " [1, 17, 9, 2, 1, 4],\n",
       " [1, 17, 9, 2, 1, 5],\n",
       " [1, 17, 9, 2, 1, 6],\n",
       " [1, 17, 9, 2, 1, 7],\n",
       " [1, 17, 10, 2, 1, 4],\n",
       " [1, 17, 10, 2, 1, 5],\n",
       " [1, 17, 10, 2, 1, 6],\n",
       " [1, 17, 10, 2, 1, 7],\n",
       " [1, 18, 3, 2, 1, 4],\n",
       " [1, 18, 3, 2, 1, 5],\n",
       " [1, 18, 3, 2, 1, 6],\n",
       " [1, 18, 3, 2, 1, 7],\n",
       " [1, 18, 8, 2, 1, 4],\n",
       " [1, 18, 8, 2, 1, 5],\n",
       " [1, 18, 8, 2, 1, 6],\n",
       " [1, 18, 8, 2, 1, 7],\n",
       " [1, 18, 9, 2, 1, 4],\n",
       " [1, 18, 9, 2, 1, 5],\n",
       " [1, 18, 9, 2, 1, 6],\n",
       " [1, 18, 9, 2, 1, 7],\n",
       " [1, 18, 10, 2, 1, 4],\n",
       " [1, 18, 10, 2, 1, 5],\n",
       " [1, 18, 10, 2, 1, 6],\n",
       " [1, 18, 10, 2, 1, 7]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 11,  3,  2,  1,  4,  1, 11,  3,  2,  1,  5,  1, 11,  3,  2,  1,\n",
       "        6,  1, 11,  3,  2,  1,  7,  1, 11,  8,  2,  1,  4,  1, 11,  8,  2,\n",
       "        1,  5,  1, 11,  8,  2,  1,  6,  1, 11,  8,  2,  1,  7,  1, 11,  9,\n",
       "        2,  1,  4,  1, 11,  9,  2,  1,  5,  1, 11,  9,  2,  1,  6,  1, 11,\n",
       "        9,  2,  1,  7,  1, 11, 10,  2,  1,  4,  1, 11, 10,  2,  1,  5,  1,\n",
       "       11, 10,  2,  1,  6,  1, 11, 10,  2,  1,  7,  1, 12,  3,  2,  1,  4,\n",
       "        1, 12,  3,  2,  1,  5,  1, 12,  3,  2,  1,  6,  1, 12,  3,  2,  1,\n",
       "        7,  1, 12,  8,  2,  1,  4,  1, 12,  8,  2,  1,  5,  1, 12,  8,  2,\n",
       "        1,  6,  1, 12,  8,  2,  1,  7,  1, 12,  9,  2,  1,  4,  1, 12,  9,\n",
       "        2,  1,  5,  1, 12,  9,  2,  1,  6,  1, 12,  9,  2,  1,  7,  1, 12,\n",
       "       10,  2,  1,  4,  1, 12, 10,  2,  1,  5,  1, 12, 10,  2,  1,  6,  1,\n",
       "       12, 10,  2,  1,  7,  1, 13,  3,  2,  1,  4,  1, 13,  3,  2,  1,  5,\n",
       "        1, 13,  3,  2,  1,  6,  1, 13,  3,  2,  1,  7,  1, 13,  8,  2,  1,\n",
       "        4,  1, 13,  8,  2,  1,  5,  1, 13,  8,  2,  1,  6,  1, 13,  8,  2,\n",
       "        1,  7,  1, 13,  9,  2,  1,  4,  1, 13,  9,  2,  1,  5,  1, 13,  9,\n",
       "        2,  1,  6,  1, 13,  9,  2,  1,  7,  1, 13, 10,  2,  1,  4,  1, 13,\n",
       "       10,  2,  1,  5,  1, 13, 10,  2,  1,  6,  1, 13, 10,  2,  1,  7,  1,\n",
       "       14,  3,  2,  1,  4,  1, 14,  3,  2,  1,  5,  1, 14,  3,  2,  1,  6,\n",
       "        1, 14,  3,  2,  1,  7,  1, 14,  8,  2,  1,  4,  1, 14,  8,  2,  1,\n",
       "        5,  1, 14,  8,  2,  1,  6,  1, 14,  8,  2,  1,  7,  1, 14,  9,  2,\n",
       "        1,  4,  1, 14,  9,  2,  1,  5,  1, 14,  9,  2,  1,  6,  1, 14,  9,\n",
       "        2,  1,  7,  1, 14, 10,  2,  1,  4,  1, 14, 10,  2,  1,  5,  1, 14,\n",
       "       10,  2,  1,  6,  1, 14, 10,  2,  1,  7,  1, 15,  3,  2,  1,  4,  1,\n",
       "       15,  3,  2,  1,  5,  1, 15,  3,  2,  1,  6,  1, 15,  3,  2,  1,  7,\n",
       "        1, 15,  8,  2,  1,  4,  1, 15,  8,  2,  1,  5,  1, 15,  8,  2,  1,\n",
       "        6,  1, 15,  8,  2,  1,  7,  1, 15,  9,  2,  1,  4,  1, 15,  9,  2,\n",
       "        1,  5,  1, 15,  9,  2,  1,  6,  1, 15,  9,  2,  1,  7,  1, 15, 10,\n",
       "        2,  1,  4,  1, 15, 10,  2,  1,  5,  1, 15, 10,  2,  1,  6,  1, 15,\n",
       "       10,  2,  1,  7,  1, 16,  3,  2,  1,  4,  1, 16,  3,  2,  1,  5,  1,\n",
       "       16,  3,  2,  1,  6,  1, 16,  3,  2,  1,  7,  1, 16,  8,  2,  1,  4,\n",
       "        1, 16,  8,  2,  1,  5,  1, 16,  8,  2,  1,  6,  1, 16,  8,  2,  1,\n",
       "        7,  1, 16,  9,  2,  1,  4,  1, 16,  9,  2,  1,  5,  1, 16,  9,  2,\n",
       "        1,  6,  1, 16,  9,  2,  1,  7,  1, 16, 10,  2,  1,  4,  1, 16, 10,\n",
       "        2,  1,  5,  1, 16, 10,  2,  1,  6,  1, 16, 10,  2,  1,  7,  1, 17,\n",
       "        3,  2,  1,  4,  1, 17,  3,  2,  1,  5,  1, 17,  3,  2,  1,  6,  1,\n",
       "       17,  3,  2,  1,  7,  1, 17,  8,  2,  1,  4,  1, 17,  8,  2,  1,  5,\n",
       "        1, 17,  8,  2,  1,  6,  1, 17,  8,  2,  1,  7,  1, 17,  9,  2,  1,\n",
       "        4,  1, 17,  9,  2,  1,  5,  1, 17,  9,  2,  1,  6,  1, 17,  9,  2,\n",
       "        1,  7,  1, 17, 10,  2,  1,  4,  1, 17, 10,  2,  1,  5,  1, 17, 10,\n",
       "        2,  1,  6,  1, 17, 10,  2,  1,  7,  1, 18,  3,  2,  1,  4,  1, 18,\n",
       "        3,  2,  1,  5,  1, 18,  3,  2,  1,  6,  1, 18,  3,  2,  1,  7,  1,\n",
       "       18,  8,  2,  1,  4,  1, 18,  8,  2,  1,  5,  1, 18,  8,  2,  1,  6,\n",
       "        1, 18,  8,  2,  1,  7,  1, 18,  9,  2,  1,  4,  1, 18,  9,  2,  1,\n",
       "        5,  1, 18,  9,  2,  1,  6,  1, 18,  9,  2,  1,  7,  1, 18, 10,  2,\n",
       "        1,  4,  1, 18, 10,  2,  1,  5,  1, 18, 10,  2,  1,  6,  1, 18, 10,\n",
       "        2,  1,  7])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
